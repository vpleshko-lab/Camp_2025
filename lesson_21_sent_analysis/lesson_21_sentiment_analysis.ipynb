{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "# Text classification: sentiment analysis \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Popular tasks of text classification\n",
    "\n",
    "</font>\n",
    "\n",
    "- **Spam detection**: Having message decide is is spammy or not \n",
    "- **Topic identification**: Having article choose one of known classes like \"Sport\", \"Technology\", \"Finances\"\n",
    "- **Sentiment analysis**: Is the moview positive or negative \n",
    "- **Spelling correction**: what is more suitable \"weather\" or \"whether\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Features from Text\n",
    "\n",
    "</font>\n",
    "\n",
    "1. The most common words\n",
    "2. *Stop* words\n",
    "3. Normalization: lower case / stemming / lemmatizing\n",
    "4. Capitalization as feature \n",
    "5. POS e.g. \"the weather\" vs whether  \n",
    "6. grouping\n",
    "    - buy, purchase\n",
    "    - Mr, Ms, Dr\n",
    "    - Numbers\n",
    "    - Dates\n",
    "7. Bigrams, n-grams e.g. \"White House\"\n",
    "8. Sub-sequences e.g. \"ing\", \"ion\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Text classification of search query \n",
    "\n",
    "</font>\n",
    "\n",
    "- **python**  as snake -> Zoology\n",
    "- **python**  as programming language -> Computer Science\n",
    "- **python**  as \"monty python\" -> Entertainment\n",
    "\n",
    "Probabilistic model:\n",
    "\n",
    "#### Bayes Rule\n",
    "\n",
    "\\begin{equation*}\n",
    "P(y|X) = \\frac{P(X| y) \\cdot P(y)}{P(X)} \n",
    "\\quad\\quad\\quad\n",
    "Posterior = \\frac{ Likelihood \\cdot Prior}{Evidence} \n",
    "\\quad\\quad\\quad\n",
    "P(class| python) = \\frac{P(python| class) \\cdot P(class)}{P(python)} \n",
    "\\end{equation*}\n",
    "\n",
    "Considering the $P(python)$ is common for all classes we may compare just nominators: \n",
    "\n",
    "\\begin{equation*}\n",
    "P(python| Zoology) \\cdot P(Zoology) \n",
    "\\quad\\quad\\quad \n",
    "P(python|CS) \\cdot P(CS) \n",
    "\\quad\\quad\\quad\n",
    "P(python|Entertainment) \\cdot P(Entertainment) \n",
    "\\end{equation*}\n",
    "\n",
    "In general: \n",
    "\\begin{equation*}\n",
    "\\hat{y} =  \\underset{y}{argmax} \\quad P(y|X) =  \\underset{argmax}{y} P(X|y) \\cdot P(y)\n",
    "\\end{equation*}\n",
    "\n",
    "Most probably predicted class is <font color = blue>CS</font>\n",
    "\n",
    "#### Naive Bayes Classifiers\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat{y} =  \\underset{y}{argmax} \\quad P(y) \\prod_{ i=1 }^{ n }{ P(x_{ i }|\\,y) } \n",
    "\\end{equation*}\n",
    "\n",
    "If search query = **\"python snake\"** \n",
    "\\begin{equation*}\n",
    "\\hat{y} =  \\underset{y}{argmax} \\quad\n",
    "P(y)\\cdot P(python|\\,y) \\cdot P(snake|\\,y)\n",
    "\\end{equation*}\n",
    "\n",
    "Now, the most probably predicted class is <font color = blue>Zoology</font> since  $P(snake|\\,CS)$ is far less than $P(snake|\\,Zoology)$\n",
    "\n",
    "Note: if one of word is not presented in text then its statistical propability = 0 and as the result\n",
    "the whole likelihood = 0 regardless of other words. Thus it is worth using laplace smooting  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Laplace smooting \n",
    " \n",
    "</font>\n",
    "\n",
    "\n",
    "$\n",
    "A : 1 \\quad\n",
    "B : 3\\quad\n",
    "C : 0\\quad\n",
    "D : 6\\quad\n",
    "$\n",
    "\n",
    "$N= 10\\quad K =4$ \n",
    "<br>N - number of samples, K - number of classes\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A) = 0.1\\quad\\quad\\quad\\quad\n",
    "P(B) = 0.3\\quad\\quad\\quad\\quad\n",
    "P(C) = 0.0\\quad\\quad\\quad\\quad\n",
    "P(D) = 0.6\\\\\n",
    "\\end{equation*}\n",
    "\n",
    "<font color = blue >\n",
    "\n",
    "\\begin{equation*}\n",
    "P^{\\,L}(x_{i}) =  \\frac{P(x_{i})+1}{N+K}\n",
    "\\end{equation*}\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "P^{\\,L}(A) =  \\frac{1+1}{10+4} = 0.14 \\quad P^{\\,L}(B) =  \\frac{3+1}{10+4} = 0.29\n",
    "\\quad P^{\\,L}(C) =  \\frac{0+1}{10+4} = 0.07 \\quad P^{\\,L}(D) =  \\frac{6+1}{10+4} = 0.5\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Sentiment Analysis\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Using NLTK\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/master/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Load data\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "the happy bastard's quick movie review \n",
      "damn that y2k bug . \n",
      "it's got a head start in this movie starring jamie lee curtis and another baldwin brother ( william this time ) in a story regarding a crew of a tugboat that comes across a deserted russian tech ship that has a strangeness to it when they kick the power back on . \n",
      "little do they know the power within . . . \n",
      "going for the gore and bringing on a few action sequences here and there , virus still feels very empty , like a movie going for all flash and no substance . \n",
      "we don't know why the crew was really out in the middle of nowhere , we don't know the origin of what took over the ship ( just that a big pink flashy thing hit the mir ) , and , of course , we don't know why donald sutherland is stumbling around drunkenly throughout . \n",
      "here , it's just \" hey , let's chase these people around with some robots \" . \n",
      "the acting is below average , even from the likes of curtis . \n",
      "you're more likely to get a kick out of her work in halloween h20 . \n",
      "sutherland is wasted and baldwin , well , he's acting like a baldwin , of course . \n",
      "the real star here are stan winston's robot design , some schnazzy cgi , and the occasional good gore shot , like picking into someone's brain . \n",
      "so , if robots and body parts really turn you on , here's your movie . \n",
      "otherwise , it's pretty much a sunken ship of a movie . \n",
      "it is movies like these that make a jaded movie viewer thankful for the invention of the timex indiglo watch . \n",
      "based on the late 1960's television show by the same name , the mod squad t\n"
     ]
    }
   ],
   "source": [
    "all_movie_reviews_text= movie_reviews.raw() # it is just all reviews joined into one text e.g.\n",
    "# this is the ending of first review: \" the others ( 9/10 ) - stir of echoes ( 8/10 ) \"\n",
    "# this is the beginning of second review : \"the happy bastard's quick movie review \"\n",
    "print(all_movie_reviews_text[3600:5600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Tools to review data \n",
    "\n",
    "</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats =  movie_reviews.categories()\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg/cv000_29416.txt',\n",
       " 'neg/cv001_19502.txt',\n",
       " 'neg/cv002_17424.txt',\n",
       " 'neg/cv003_12683.txt',\n",
       " 'neg/cv004_12641.txt',\n",
       " 'neg/cv005_29357.txt',\n",
       " 'neg/cv006_17022.txt',\n",
       " 'neg/cv007_4992.txt',\n",
       " 'neg/cv008_29326.txt',\n",
       " 'neg/cv009_29417.txt']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = cats[0]\n",
    "ids= movie_reviews.fileids(cat)\n",
    "ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \n",
      "which is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway & memento ) , but there are good and bad ways of making all types of films , and these folks just didn't snag this one correctly . \n",
      "they seem to have taken this pretty neat concept , but executed it terribly . \n",
      "so what are the problems with the movie ? \n",
      "well , its main problem is that it's simply too jumbled . \n",
      "it starts off \" normal \" but then downshifts into this \" fantasy \" world in which you , as an audience member , have no idea what's going on . \n",
      "there are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . \n",
      "now i personally don't mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film's biggest problem . \n",
      "it's obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . \n",
      "and do they make things entertaining , thrilling or even engaging , in the meantime ? \n",
      "not really . \n",
      "the sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn't the make the film all that more entertaining . \n",
      "i guess the bottom line with movies like this is that you should always make sure that the audience is \" into it \" even before they are given the secret password to enter your world of understanding . \n",
      "i mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! \n",
      "okay , we get it . . . there \n",
      "are people chasing her and we don't know who they are . \n",
      "do we really need to see it over and over again ? \n",
      "how about giving us different scenes offering further insight into all of the strangeness going down in the movie ? \n",
      "apparently , the studio took this film away from its director and chopped it up themselves , and it shows . \n",
      "there might've been a pretty decent teen mind-fuck movie in here somewhere , but i guess \" the suits \" decided that turning it into a music video with little edge , would make more sense . \n",
      "the actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . \n",
      "but my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character's unraveling . \n",
      "overall , the film doesn't stick because it doesn't entertain , it's confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . \n",
      "oh , and by the way , this is not a horror or teen slasher flick . . . it's \n",
      "just packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . \n",
      "it also wrapped production two years ago and has been sitting on the shelves ever since . \n",
      "whatever . . . skip \n",
      "it ! \n",
      "where's joblo coming from ? \n",
      "a nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "id_review = ids[0]\n",
    "print(movie_reviews.raw(id_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Tokenize\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336782\n",
      "['plot', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', 'drink', 'and', 'then', 'drive', 'they', 'get', 'into', 'an', 'accident', 'one', 'of', 'the', 'guys', 'dies', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', 'and', 'has', 'nightmares', 'what', 's', 'the', 'deal', 'watch', 'the', 'movie', 'and', 'sorta', 'find', 'out', 'critique', 'a', 'mind', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', 'mess', 'with', 'your', 'head', 'and', 'such']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text): # removes punctualtion\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # just for demo\n",
    "    return tokenizer.tokenize(text.lower())\n",
    "\n",
    "all_words = preprocess(all_movie_reviews_text)\n",
    "print (len(all_words))\n",
    "print(all_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Build vocabulary\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FreqDist in module nltk.probability:\n",
      "\n",
      "class FreqDist(collections.Counter)\n",
      " |  FreqDist(samples=None)\n",
      " |\n",
      " |  A frequency distribution for the outcomes of an experiment.  A\n",
      " |  frequency distribution records the number of times each outcome of\n",
      " |  an experiment has occurred.  For example, a frequency distribution\n",
      " |  could be used to record the frequency of each word type in a\n",
      " |  document.  Formally, a frequency distribution can be defined as a\n",
      " |  function mapping from each sample to the number of times that\n",
      " |  sample occurred as an outcome.\n",
      " |\n",
      " |  Frequency distributions are generally constructed by running a\n",
      " |  number of experiments, and incrementing the count for a sample\n",
      " |  every time it is an outcome of an experiment.  For example, the\n",
      " |  following code will produce a frequency distribution that encodes\n",
      " |  how often each word occurs in a text:\n",
      " |\n",
      " |      >>> from nltk.tokenize import word_tokenize\n",
      " |      >>> from nltk.probability import FreqDist\n",
      " |      >>> sent = 'This is an example sentence'\n",
      " |      >>> fdist = FreqDist()\n",
      " |      >>> for word in word_tokenize(sent):\n",
      " |      ...    fdist[word.lower()] += 1\n",
      " |\n",
      " |  An equivalent way to do this is with the initializer:\n",
      " |\n",
      " |      >>> fdist = FreqDist(word.lower() for word in word_tokenize(sent))\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      FreqDist\n",
      " |      collections.Counter\n",
      " |      builtins.dict\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  B(self)\n",
      " |      Return the total number of sample values (or \"bins\") that\n",
      " |      have counts greater than zero.  For the total\n",
      " |      number of sample outcomes recorded, use ``FreqDist.N()``.\n",
      " |      (FreqDist.B() is the same as len(FreqDist).)\n",
      " |\n",
      " |      :rtype: int\n",
      " |\n",
      " |  N(self)\n",
      " |      Return the total number of sample outcomes that have been\n",
      " |      recorded by this FreqDist.  For the number of unique\n",
      " |      sample values (or bins) with counts greater than zero, use\n",
      " |      ``FreqDist.B()``.\n",
      " |\n",
      " |      :rtype: int\n",
      " |\n",
      " |  Nr(self, r, bins=None)\n",
      " |\n",
      " |  __add__(self, other)\n",
      " |      Add counts from two counters.\n",
      " |\n",
      " |      >>> FreqDist('abbb') + FreqDist('bcc')\n",
      " |      FreqDist({'b': 4, 'c': 2, 'a': 1})\n",
      " |\n",
      " |  __and__(self, other)\n",
      " |      Intersection is the minimum of corresponding counts.\n",
      " |\n",
      " |      >>> FreqDist('abbb') & FreqDist('bcc')\n",
      " |      FreqDist({'b': 1})\n",
      " |\n",
      " |  __delitem__(self, key)\n",
      " |      Override ``Counter.__delitem__()`` to invalidate the cached N\n",
      " |\n",
      " |  __ge__(self, other)\n",
      " |      True if all counts in self are a superset of those in other.\n",
      " |\n",
      " |  __gt__ lambda self, other\n",
      " |\n",
      " |  __init__(self, samples=None)\n",
      " |      Construct a new frequency distribution.  If ``samples`` is\n",
      " |      given, then the frequency distribution will be initialized\n",
      " |      with the count of each object in ``samples``; otherwise, it\n",
      " |      will be initialized to be empty.\n",
      " |\n",
      " |      In particular, ``FreqDist()`` returns an empty frequency\n",
      " |      distribution; and ``FreqDist(samples)`` first creates an empty\n",
      " |      frequency distribution, and then calls ``update`` with the\n",
      " |      list ``samples``.\n",
      " |\n",
      " |      :param samples: The samples to initialize the frequency\n",
      " |          distribution with.\n",
      " |      :type samples: Sequence\n",
      " |\n",
      " |  __iter__(self)\n",
      " |      Return an iterator which yields tokens ordered by frequency.\n",
      " |\n",
      " |      :rtype: iterator\n",
      " |\n",
      " |  __le__(self, other)\n",
      " |      Returns True if this frequency distribution is a subset of the other\n",
      " |      and for no key the value exceeds the value of the same key from\n",
      " |      the other frequency distribution.\n",
      " |\n",
      " |      The <= operator forms partial order and satisfying the axioms\n",
      " |      reflexivity, antisymmetry and transitivity.\n",
      " |\n",
      " |      >>> FreqDist('a') <= FreqDist('a')\n",
      " |      True\n",
      " |      >>> a = FreqDist('abc')\n",
      " |      >>> b = FreqDist('aabc')\n",
      " |      >>> (a <= b, b <= a)\n",
      " |      (True, False)\n",
      " |      >>> FreqDist('a') <= FreqDist('abcd')\n",
      " |      True\n",
      " |      >>> FreqDist('abc') <= FreqDist('xyz')\n",
      " |      False\n",
      " |      >>> FreqDist('xyz') <= FreqDist('abc')\n",
      " |      False\n",
      " |      >>> c = FreqDist('a')\n",
      " |      >>> d = FreqDist('aa')\n",
      " |      >>> e = FreqDist('aaa')\n",
      " |      >>> c <= d and d <= e and c <= e\n",
      " |      True\n",
      " |\n",
      " |  __lt__ lambda self, other\n",
      " |\n",
      " |  __or__(self, other)\n",
      " |      Union is the maximum of value in either of the input counters.\n",
      " |\n",
      " |      >>> FreqDist('abbb') | FreqDist('bcc')\n",
      " |      FreqDist({'b': 3, 'c': 2, 'a': 1})\n",
      " |\n",
      " |  __repr__(self)\n",
      " |      Return a string representation of this FreqDist.\n",
      " |\n",
      " |      :rtype: string\n",
      " |\n",
      " |  __setitem__(self, key, val)\n",
      " |      Override ``Counter.__setitem__()`` to invalidate the cached N\n",
      " |\n",
      " |  __str__(self)\n",
      " |      Return a string representation of this FreqDist.\n",
      " |\n",
      " |      :rtype: string\n",
      " |\n",
      " |  __sub__(self, other)\n",
      " |      Subtract count, but keep only results with positive counts.\n",
      " |\n",
      " |      >>> FreqDist('abbbc') - FreqDist('bccd')\n",
      " |      FreqDist({'b': 2, 'a': 1})\n",
      " |\n",
      " |  copy(self)\n",
      " |      Create a copy of this frequency distribution.\n",
      " |\n",
      " |      :rtype: FreqDist\n",
      " |\n",
      " |  freq(self, sample)\n",
      " |      Return the frequency of a given sample.  The frequency of a\n",
      " |      sample is defined as the count of that sample divided by the\n",
      " |      total number of sample outcomes that have been recorded by\n",
      " |      this FreqDist.  The count of a sample is defined as the\n",
      " |      number of times that sample outcome was recorded by this\n",
      " |      FreqDist.  Frequencies are always real numbers in the range\n",
      " |      [0, 1].\n",
      " |\n",
      " |      :param sample: the sample whose frequency\n",
      " |             should be returned.\n",
      " |      :type sample: any\n",
      " |      :rtype: float\n",
      " |\n",
      " |  hapaxes(self)\n",
      " |      Return a list of all samples that occur once (hapax legomena)\n",
      " |\n",
      " |      :rtype: list\n",
      " |\n",
      " |  max(self)\n",
      " |      Return the sample with the greatest number of outcomes in this\n",
      " |      frequency distribution.  If two or more samples have the same\n",
      " |      number of outcomes, return one of them; which sample is\n",
      " |      returned is undefined.  If no outcomes have occurred in this\n",
      " |      frequency distribution, return None.\n",
      " |\n",
      " |      :return: The sample with the maximum number of outcomes in this\n",
      " |              frequency distribution.\n",
      " |      :rtype: any or None\n",
      " |\n",
      " |  pformat(self, maxlen=10)\n",
      " |      Return a string representation of this FreqDist.\n",
      " |\n",
      " |      :param maxlen: The maximum number of items to display\n",
      " |      :type maxlen: int\n",
      " |      :rtype: string\n",
      " |\n",
      " |  plot(self, *args, title='', cumulative=False, percents=False, show=False, **kwargs)\n",
      " |      Plot samples from the frequency distribution\n",
      " |      displaying the most frequent sample first.  If an integer\n",
      " |      parameter is supplied, stop after this many samples have been\n",
      " |      plotted.  For a cumulative plot, specify cumulative=True. Additional\n",
      " |      ``**kwargs`` are passed to matplotlib's plot function.\n",
      " |      (Requires Matplotlib to be installed.)\n",
      " |\n",
      " |      :param title: The title for the graph.\n",
      " |      :type title: str\n",
      " |      :param cumulative: Whether the plot is cumulative. (default = False)\n",
      " |      :type cumulative: bool\n",
      " |      :param percents: Whether the plot uses percents instead of counts. (default = False)\n",
      " |      :type percents: bool\n",
      " |      :param show: Whether to show the plot, or only return the ax.\n",
      " |      :type show: bool\n",
      " |\n",
      " |  pprint(self, maxlen=10, stream=None)\n",
      " |      Print a string representation of this FreqDist to 'stream'\n",
      " |\n",
      " |      :param maxlen: The maximum number of items to print\n",
      " |      :type maxlen: int\n",
      " |      :param stream: The stream to print to. stdout by default\n",
      " |\n",
      " |  r_Nr(self, bins=None)\n",
      " |      Return the dictionary mapping r to Nr, the number of samples with frequency r, where Nr > 0.\n",
      " |\n",
      " |      :type bins: int\n",
      " |      :param bins: The number of possible sample outcomes.  ``bins``\n",
      " |          is used to calculate Nr(0).  In particular, Nr(0) is\n",
      " |          ``bins-self.B()``.  If ``bins`` is not specified, it\n",
      " |          defaults to ``self.B()`` (so Nr(0) will be 0).\n",
      " |      :rtype: int\n",
      " |\n",
      " |  setdefault(self, key, val)\n",
      " |      Override ``Counter.setdefault()`` to invalidate the cached N\n",
      " |\n",
      " |  tabulate(self, *args, **kwargs)\n",
      " |      Tabulate the given samples from the frequency distribution (cumulative),\n",
      " |      displaying the most frequent sample first.  If an integer\n",
      " |      parameter is supplied, stop after this many samples have been\n",
      " |      plotted.\n",
      " |\n",
      " |      :param samples: The samples to plot (default is all samples)\n",
      " |      :type samples: list\n",
      " |      :param cumulative: A flag to specify whether the freqs are cumulative (default = False)\n",
      " |      :type title: bool\n",
      " |\n",
      " |  update(self, *args, **kwargs)\n",
      " |      Override ``Counter.update()`` to invalidate the cached N\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from collections.Counter:\n",
      " |\n",
      " |  __eq__(self, other)\n",
      " |      True if all counts agree. Missing counts are treated as zero.\n",
      " |\n",
      " |  __iadd__(self, other)\n",
      " |      Inplace add from another counter, keeping only positive counts.\n",
      " |\n",
      " |      >>> c = Counter('abbb')\n",
      " |      >>> c += Counter('bcc')\n",
      " |      >>> c\n",
      " |      Counter({'b': 4, 'c': 2, 'a': 1})\n",
      " |\n",
      " |  __iand__(self, other)\n",
      " |      Inplace intersection is the minimum of corresponding counts.\n",
      " |\n",
      " |      >>> c = Counter('abbb')\n",
      " |      >>> c &= Counter('bcc')\n",
      " |      >>> c\n",
      " |      Counter({'b': 1})\n",
      " |\n",
      " |  __ior__(self, other)\n",
      " |      Inplace union is the maximum of value from either counter.\n",
      " |\n",
      " |      >>> c = Counter('abbb')\n",
      " |      >>> c |= Counter('bcc')\n",
      " |      >>> c\n",
      " |      Counter({'b': 3, 'c': 2, 'a': 1})\n",
      " |\n",
      " |  __isub__(self, other)\n",
      " |      Inplace subtract counter, but keep only results with positive counts.\n",
      " |\n",
      " |      >>> c = Counter('abbbc')\n",
      " |      >>> c -= Counter('bccd')\n",
      " |      >>> c\n",
      " |      Counter({'b': 2, 'a': 1})\n",
      " |\n",
      " |  __missing__(self, key)\n",
      " |      The count of elements not in the Counter is zero.\n",
      " |\n",
      " |  __ne__(self, other)\n",
      " |      True if any counts disagree. Missing counts are treated as zero.\n",
      " |\n",
      " |  __neg__(self)\n",
      " |      Subtracts from an empty counter.  Strips positive and zero counts,\n",
      " |      and flips the sign on negative counts.\n",
      " |\n",
      " |  __pos__(self)\n",
      " |      Adds an empty counter, effectively stripping negative and zero counts\n",
      " |\n",
      " |  __reduce__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  elements(self)\n",
      " |      Iterator over elements repeating each as many times as its count.\n",
      " |\n",
      " |      >>> c = Counter('ABCABC')\n",
      " |      >>> sorted(c.elements())\n",
      " |      ['A', 'A', 'B', 'B', 'C', 'C']\n",
      " |\n",
      " |      Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n",
      " |\n",
      " |      >>> import math\n",
      " |      >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n",
      " |      >>> math.prod(prime_factors.elements())\n",
      " |      1836\n",
      " |\n",
      " |      Note, if an element's count has been set to zero or is a negative\n",
      " |      number, elements() will ignore it.\n",
      " |\n",
      " |  most_common(self, n=None)\n",
      " |      List the n most common elements and their counts from the most\n",
      " |      common to the least.  If n is None, then list all element counts.\n",
      " |\n",
      " |      >>> Counter('abracadabra').most_common(3)\n",
      " |      [('a', 5), ('b', 2), ('r', 2)]\n",
      " |\n",
      " |  subtract(self, iterable=None, /, **kwds)\n",
      " |      Like dict.update() but subtracts counts instead of replacing them.\n",
      " |      Counts can be reduced below zero.  Both the inputs and outputs are\n",
      " |      allowed to contain zero and negative counts.\n",
      " |\n",
      " |      Source can be an iterable, a dictionary, or another Counter instance.\n",
      " |\n",
      " |      >>> c = Counter('which')\n",
      " |      >>> c.subtract('witch')             # subtract elements from another iterable\n",
      " |      >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n",
      " |      >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n",
      " |      0\n",
      " |      >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n",
      " |      -1\n",
      " |\n",
      " |  total(self)\n",
      " |      Sum of the counts\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from collections.Counter:\n",
      " |\n",
      " |  fromkeys(iterable, v=None)\n",
      " |      Create a new dictionary with keys from iterable and values set to value.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from collections.Counter:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from collections.Counter:\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from builtins.dict:\n",
      " |\n",
      " |  __contains__(self, key, /)\n",
      " |      True if the dictionary has the specified key, else False.\n",
      " |\n",
      " |  __getattribute__(self, name, /)\n",
      " |      Return getattr(self, name).\n",
      " |\n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |\n",
      " |  __len__(self, /)\n",
      " |      Return len(self).\n",
      " |\n",
      " |  __reversed__(self, /)\n",
      " |      Return a reverse iterator over the dict keys.\n",
      " |\n",
      " |  __ror__(self, value, /)\n",
      " |      Return value|self.\n",
      " |\n",
      " |  __sizeof__(...)\n",
      " |      D.__sizeof__() -> size of D in memory, in bytes\n",
      " |\n",
      " |  clear(...)\n",
      " |      D.clear() -> None.  Remove all items from D.\n",
      " |\n",
      " |  get(self, key, default=None, /)\n",
      " |      Return the value for key if key is in the dictionary, else default.\n",
      " |\n",
      " |  items(...)\n",
      " |      D.items() -> a set-like object providing a view on D's items\n",
      " |\n",
      " |  keys(...)\n",
      " |      D.keys() -> a set-like object providing a view on D's keys\n",
      " |\n",
      " |  pop(...)\n",
      " |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      " |\n",
      " |      If the key is not found, return the default if given; otherwise,\n",
      " |      raise a KeyError.\n",
      " |\n",
      " |  popitem(self, /)\n",
      " |      Remove and return a (key, value) pair as a 2-tuple.\n",
      " |\n",
      " |      Pairs are returned in LIFO (last-in, first-out) order.\n",
      " |      Raises KeyError if the dict is empty.\n",
      " |\n",
      " |  values(...)\n",
      " |      D.values() -> an object providing a view on D's values\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from builtins.dict:\n",
      " |\n",
      " |  __class_getitem__(...)\n",
      " |      See PEP 585\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from builtins.dict:\n",
      " |\n",
      " |  __new__(*args, **kwargs) class method of builtins.dict\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.FreqDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of vocabulary: 39,696\n",
      "('the', 'a', 'and', 'of', 'to', 'is', 'in', 's', 'it', 'that', 'as', 'with', 'for', 'his', 'this', 'film', 'i', 'he', 'but', 'on', 'are', 't', 'by', 'be', 'one', 'movie', 'an', 'who', 'not', 'you', 'from', 'at', 'was', 'have', 'they', 'has', 'her', 'all', 'there', 'like', 'so', 'out', 'about', 'up', 'more', 'what', 'when', 'which', 'or', 'she', 'their', 'some', 'just', 'can', 'if', 'we', 'him', 'into', 'even', 'only', 'than', 'no', 'good', 'time', 'most', 'its', 'will', 'story', 'would', 'been', 'much', 'character', 'also', 'get', 'other', 'do', 'two', 'well', 'them', 'very', 'characters', 'first', 'after', 'see', 'way', 'because', 'make', 'life', 'off', 'too', 'any', 'does', 'really', 'had', 'while', 'films', 'how', 'plot', 'little', 'where')\n"
     ]
    }
   ],
   "source": [
    "all_words=nltk.FreqDist(all_words)\n",
    "print ('len of vocabulary: {:,}'.format (len(all_words)))\n",
    "# Use most common words\n",
    "most_common_words = list(zip(*all_words.most_common()))[0] # [0] means names whereas [1] are frequencies\n",
    "print (most_common_words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Get rid of stop words \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/master/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(words):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [w for w in words if w not in stop_words]\n",
    "most_common_words_filtered = remove_stop_words(most_common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Select features \n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['film', 'one', 'movie', 'like', 'even', 'good', 'time', 'story', 'would', 'much', 'character', 'also', 'get', 'two', 'well', 'characters', 'first', 'see', 'way', 'make', 'life', 'really', 'films', 'plot', 'little', 'people', 'could', 'scene', 'man', 'bad', 'never', 'best', 'new', 'scenes', 'many', 'director', 'know', 'movies', 'action', 'great', 'another', 'love', 'go', 'made', 'us', 'big', 'end', 'something', 'back', 'still', 'world', 'seems', 'work', 'makes', 'however', 'every', 'though', 'better', 'real', 'audience', 'enough', 'seen', 'take', 'around', 'going', 'year', 'performance', 'role', 'old', 'gets', 'may', 'things', 'think', 'years', 'last', 'comedy', 'funny', 'actually', 'long', 'look', 'almost', 'thing', 'fact', 'nothing', 'say', 'right', 'john', 'although', 'played', 'find', 'script', 'come', 'ever', 'cast', 'since', 'star', 'plays', 'young', 'show', 'comes']\n"
     ]
    }
   ],
   "source": [
    "word_features = most_common_words_filtered [:3000]\n",
    "print (word_features[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Extract documents and labels\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: this does not use tokenizing to documents but words of document retrieved by file_id instead.\n",
    "documents = [(list(movie_reviews.words(file_id)), category) # using the words() method of movie_reviews object\n",
    "            for category in movie_reviews.categories() # select category - there are two: ['neg', 'pos']\n",
    "            for file_id in movie_reviews.fileids(category)]# select all file_ids for specified category\n",
    "len (documents)\n",
    "# This returns list of tuples (list_of_tokens_of document, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an', 'accident', '.', 'one', 'of', 'the', 'guys', 'dies', ',', 'but', 'his', 'girlfriend', 'continues', 'to', 'see', 'him', 'in', 'her', 'life', ',', 'and', 'has', 'nightmares', '.', 'what', \"'\", 's', 'the', 'deal', '?', 'watch', 'the', 'movie', 'and', '\"', 'sorta', '\"', 'find', 'out', '.', '.', '.', 'critique', ':', 'a', 'mind', '-', 'fuck', 'movie', 'for', 'the', 'teen', 'generation', 'that', 'touches', 'on', 'a', 'very', 'cool', 'idea', ',', 'but', 'presents', 'it', 'in', 'a', 'very', 'bad', 'package', '.', 'which', 'is', 'what', 'makes', 'this', 'review', 'an', 'even', 'harder', 'one', 'to', 'write', ',', 'since', 'i', 'generally', 'applaud', 'films', 'which', 'attempt', 'to', 'break', 'the', 'mold', ',', 'mess', 'with', 'your', 'head', 'and', 'such', '(', 'lost', 'highway', '&', 'memento', ')', ',', 'but', 'there', 'are', 'good', 'and', 'bad', 'ways', 'of', 'making', 'all', 'types', 'of', 'films', ',', 'and', 'these', 'folks', 'just', 'didn', \"'\", 't', 'snag', 'this', 'one', 'correctly', '.', 'they', 'seem', 'to', 'have', 'taken', 'this', 'pretty', 'neat', 'concept', ',', 'but', 'executed', 'it', 'terribly', '.', 'so', 'what', 'are', 'the', 'problems', 'with', 'the', 'movie', '?', 'well', ',', 'its', 'main', 'problem', 'is', 'that', 'it', \"'\", 's', 'simply', 'too', 'jumbled', '.', 'it', 'starts', 'off', '\"', 'normal', '\"', 'but', 'then', 'downshifts', 'into', 'this', '\"', 'fantasy', '\"', 'world', 'in', 'which', 'you', ',', 'as', 'an', 'audience', 'member', ',', 'have', 'no', 'idea', 'what', \"'\", 's', 'going', 'on', '.', 'there', 'are', 'dreams', ',', 'there', 'are', 'characters', 'coming', 'back', 'from', 'the', 'dead', ',', 'there', 'are', 'others', 'who', 'look', 'like', 'the', 'dead', ',', 'there', 'are', 'strange', 'apparitions', ',', 'there', 'are', 'disappearances', ',', 'there', 'are', 'a', 'looooot', 'of', 'chase', 'scenes', ',', 'there', 'are', 'tons', 'of', 'weird', 'things', 'that', 'happen', ',', 'and', 'most', 'of', 'it', 'is', 'simply', 'not', 'explained', '.', 'now', 'i', 'personally', 'don', \"'\", 't', 'mind', 'trying', 'to', 'unravel', 'a', 'film', 'every', 'now', 'and', 'then', ',', 'but', 'when', 'all', 'it', 'does', 'is', 'give', 'me', 'the', 'same', 'clue', 'over', 'and', 'over', 'again', ',', 'i', 'get', 'kind', 'of', 'fed', 'up', 'after', 'a', 'while', ',', 'which', 'is', 'this', 'film', \"'\", 's', 'biggest', 'problem', '.', 'it', \"'\", 's', 'obviously', 'got', 'this', 'big', 'secret', 'to', 'hide', ',', 'but', 'it', 'seems', 'to', 'want', 'to', 'hide', 'it', 'completely', 'until', 'its', 'final', 'five', 'minutes', '.', 'and', 'do', 'they', 'make', 'things', 'entertaining', ',', 'thrilling', 'or', 'even', 'engaging', ',', 'in', 'the', 'meantime', '?', 'not', 'really', '.', 'the', 'sad', 'part', 'is', 'that', 'the', 'arrow', 'and', 'i', 'both', 'dig', 'on', 'flicks', 'like', 'this', ',', 'so', 'we', 'actually', 'figured', 'most', 'of', 'it', 'out', 'by', 'the', 'half', '-', 'way', 'point', ',', 'so', 'all', 'of', 'the', 'strangeness', 'after', 'that', 'did', 'start', 'to', 'make', 'a', 'little', 'bit', 'of', 'sense', ',', 'but', 'it', 'still', 'didn', \"'\", 't', 'the', 'make', 'the', 'film', 'all', 'that', 'more', 'entertaining', '.', 'i', 'guess', 'the', 'bottom', 'line', 'with', 'movies', 'like', 'this', 'is', 'that', 'you', 'should', 'always', 'make', 'sure', 'that', 'the', 'audience', 'is', '\"', 'into', 'it', '\"', 'even', 'before', 'they', 'are', 'given', 'the', 'secret', 'password', 'to', 'enter', 'your', 'world', 'of', 'understanding', '.', 'i', 'mean', ',', 'showing', 'melissa', 'sagemiller', 'running', 'away', 'from', 'visions', 'for', 'about', '20', 'minutes', 'throughout', 'the', 'movie', 'is', 'just', 'plain', 'lazy', '!', '!', 'okay', ',', 'we', 'get', 'it', '.', '.', '.', 'there', 'are', 'people', 'chasing', 'her', 'and', 'we', 'don', \"'\", 't', 'know', 'who', 'they', 'are', '.', 'do', 'we', 'really', 'need', 'to', 'see', 'it', 'over', 'and', 'over', 'again', '?', 'how', 'about', 'giving', 'us', 'different', 'scenes', 'offering', 'further', 'insight', 'into', 'all', 'of', 'the', 'strangeness', 'going', 'down', 'in', 'the', 'movie', '?', 'apparently', ',', 'the', 'studio', 'took', 'this', 'film', 'away', 'from', 'its', 'director', 'and', 'chopped', 'it', 'up', 'themselves', ',', 'and', 'it', 'shows', '.', 'there', 'might', \"'\", 've', 'been', 'a', 'pretty', 'decent', 'teen', 'mind', '-', 'fuck', 'movie', 'in', 'here', 'somewhere', ',', 'but', 'i', 'guess', '\"', 'the', 'suits', '\"', 'decided', 'that', 'turning', 'it', 'into', 'a', 'music', 'video', 'with', 'little', 'edge', ',', 'would', 'make', 'more', 'sense', '.', 'the', 'actors', 'are', 'pretty', 'good', 'for', 'the', 'most', 'part', ',', 'although', 'wes', 'bentley', 'just', 'seemed', 'to', 'be', 'playing', 'the', 'exact', 'same', 'character', 'that', 'he', 'did', 'in', 'american', 'beauty', ',', 'only', 'in', 'a', 'new', 'neighborhood', '.', 'but', 'my', 'biggest', 'kudos', 'go', 'out', 'to', 'sagemiller', ',', 'who', 'holds', 'her', 'own', 'throughout', 'the', 'entire', 'film', ',', 'and', 'actually', 'has', 'you', 'feeling', 'her', 'character', \"'\", 's', 'unraveling', '.', 'overall', ',', 'the', 'film', 'doesn', \"'\", 't', 'stick', 'because', 'it', 'doesn', \"'\", 't', 'entertain', ',', 'it', \"'\", 's', 'confusing', ',', 'it', 'rarely', 'excites', 'and', 'it', 'feels', 'pretty', 'redundant', 'for', 'most', 'of', 'its', 'runtime', ',', 'despite', 'a', 'pretty', 'cool', 'ending', 'and', 'explanation', 'to', 'all', 'of', 'the', 'craziness', 'that', 'came', 'before', 'it', '.', 'oh', ',', 'and', 'by', 'the', 'way', ',', 'this', 'is', 'not', 'a', 'horror', 'or', 'teen', 'slasher', 'flick', '.', '.', '.', 'it', \"'\", 's', 'just', 'packaged', 'to', 'look', 'that', 'way', 'because', 'someone', 'is', 'apparently', 'assuming', 'that', 'the', 'genre', 'is', 'still', 'hot', 'with', 'the', 'kids', '.', 'it', 'also', 'wrapped', 'production', 'two', 'years', 'ago', 'and', 'has', 'been', 'sitting', 'on', 'the', 'shelves', 'ever', 'since', '.', 'whatever', '.', '.', '.', 'skip', 'it', '!', 'where', \"'\", 's', 'joblo', 'coming', 'from', '?', 'a', 'nightmare', 'of', 'elm', 'street', '3', '(', '7', '/', '10', ')', '-', 'blair', 'witch', '2', '(', '7', '/', '10', ')', '-', 'the', 'crow', '(', '9', '/', '10', ')', '-', 'the', 'crow', ':', 'salvation', '(', '4', '/', '10', ')', '-', 'lost', 'highway', '(', '10', '/', '10', ')', '-', 'memento', '(', '10', '/', '10', ')', '-', 'the', 'others', '(', '9', '/', '10', ')', '-', 'stir', 'of', 'echoes', '(', '8', '/', '10', ')'], 'neg')\n"
     ]
    }
   ],
   "source": [
    "print (documents [0]) # (['plot', ':', 'two', 'teen', ... 'echoes', '(', '8', '/', '10', ')'], 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Shuffle documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle first\n",
    "random.shuffle(documents) # it is inplace method\n",
    "documents= documents[:500] # reduce the data set for speed up the demo\n",
    "len (documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Vectorize documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(review_tokens):\n",
    "    return {w: w in set(review_tokens) for w in word_features} # feature representation on document\n",
    "\n",
    "data_set= [(find_features(review_tokens), category) for (review_tokens, category) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'film': False,\n",
       "  'one': True,\n",
       "  'movie': True,\n",
       "  'like': True,\n",
       "  'even': True,\n",
       "  'good': False,\n",
       "  'time': False,\n",
       "  'story': True,\n",
       "  'would': True,\n",
       "  'much': True,\n",
       "  'character': True,\n",
       "  'also': False,\n",
       "  'get': True,\n",
       "  'two': True,\n",
       "  'well': True,\n",
       "  'characters': False,\n",
       "  'first': True,\n",
       "  'see': True,\n",
       "  'way': True,\n",
       "  'make': False,\n",
       "  'life': False,\n",
       "  'really': True,\n",
       "  'films': False,\n",
       "  'plot': True,\n",
       "  'little': True,\n",
       "  'people': True,\n",
       "  'could': True,\n",
       "  'scene': False,\n",
       "  'man': False,\n",
       "  'bad': True,\n",
       "  'never': False,\n",
       "  'best': True,\n",
       "  'new': False,\n",
       "  'scenes': False,\n",
       "  'many': True,\n",
       "  'director': False,\n",
       "  'know': True,\n",
       "  'movies': True,\n",
       "  'action': False,\n",
       "  'great': False,\n",
       "  'another': True,\n",
       "  'love': False,\n",
       "  'go': True,\n",
       "  'made': False,\n",
       "  'us': False,\n",
       "  'big': True,\n",
       "  'end': True,\n",
       "  'something': True,\n",
       "  'back': False,\n",
       "  'still': False,\n",
       "  'world': False,\n",
       "  'seems': True,\n",
       "  'work': False,\n",
       "  'makes': False,\n",
       "  'however': False,\n",
       "  'every': False,\n",
       "  'though': False,\n",
       "  'better': True,\n",
       "  'real': True,\n",
       "  'audience': True,\n",
       "  'enough': True,\n",
       "  'seen': False,\n",
       "  'take': False,\n",
       "  'around': False,\n",
       "  'going': True,\n",
       "  'year': False,\n",
       "  'performance': False,\n",
       "  'role': True,\n",
       "  'old': False,\n",
       "  'gets': False,\n",
       "  'may': False,\n",
       "  'things': True,\n",
       "  'think': True,\n",
       "  'years': False,\n",
       "  'last': False,\n",
       "  'comedy': True,\n",
       "  'funny': True,\n",
       "  'actually': True,\n",
       "  'long': False,\n",
       "  'look': True,\n",
       "  'almost': False,\n",
       "  'thing': True,\n",
       "  'fact': False,\n",
       "  'nothing': False,\n",
       "  'say': True,\n",
       "  'right': False,\n",
       "  'john': False,\n",
       "  'although': True,\n",
       "  'played': False,\n",
       "  'find': False,\n",
       "  'script': False,\n",
       "  'come': False,\n",
       "  'ever': False,\n",
       "  'cast': False,\n",
       "  'since': False,\n",
       "  'star': True,\n",
       "  'plays': False,\n",
       "  'young': False,\n",
       "  'show': True,\n",
       "  'comes': False,\n",
       "  'part': False,\n",
       "  'original': False,\n",
       "  'actors': True,\n",
       "  'screen': True,\n",
       "  'without': False,\n",
       "  'acting': True,\n",
       "  'three': False,\n",
       "  'day': False,\n",
       "  'point': False,\n",
       "  'lot': False,\n",
       "  'least': True,\n",
       "  'takes': False,\n",
       "  'guy': False,\n",
       "  'quite': True,\n",
       "  'away': False,\n",
       "  'family': False,\n",
       "  'effects': False,\n",
       "  'course': False,\n",
       "  'goes': True,\n",
       "  'minutes': False,\n",
       "  'interesting': True,\n",
       "  'might': True,\n",
       "  'far': False,\n",
       "  'high': False,\n",
       "  'rather': True,\n",
       "  'must': True,\n",
       "  'anything': True,\n",
       "  'place': False,\n",
       "  'set': False,\n",
       "  'yet': False,\n",
       "  'watch': False,\n",
       "  'making': False,\n",
       "  'wife': False,\n",
       "  'hard': True,\n",
       "  'always': False,\n",
       "  'fun': False,\n",
       "  'seem': False,\n",
       "  'special': False,\n",
       "  'bit': True,\n",
       "  'times': False,\n",
       "  'trying': True,\n",
       "  'hollywood': False,\n",
       "  'instead': False,\n",
       "  'give': False,\n",
       "  'want': False,\n",
       "  'picture': False,\n",
       "  'kind': False,\n",
       "  'american': True,\n",
       "  'job': True,\n",
       "  'sense': False,\n",
       "  'woman': False,\n",
       "  'home': False,\n",
       "  'series': True,\n",
       "  'actor': True,\n",
       "  'probably': True,\n",
       "  'help': True,\n",
       "  'half': False,\n",
       "  'along': False,\n",
       "  'men': False,\n",
       "  'everything': False,\n",
       "  'pretty': False,\n",
       "  'becomes': False,\n",
       "  'sure': True,\n",
       "  'black': False,\n",
       "  'together': False,\n",
       "  'dialogue': True,\n",
       "  'money': True,\n",
       "  'become': False,\n",
       "  'gives': False,\n",
       "  'given': False,\n",
       "  'looking': False,\n",
       "  'whole': True,\n",
       "  'watching': False,\n",
       "  'father': False,\n",
       "  'feel': False,\n",
       "  'everyone': False,\n",
       "  'music': False,\n",
       "  'wants': False,\n",
       "  'sex': False,\n",
       "  'less': False,\n",
       "  'done': True,\n",
       "  'horror': False,\n",
       "  'got': False,\n",
       "  'death': False,\n",
       "  'perhaps': False,\n",
       "  'city': False,\n",
       "  'next': False,\n",
       "  'especially': False,\n",
       "  'play': False,\n",
       "  'girl': False,\n",
       "  'mind': False,\n",
       "  '10': False,\n",
       "  'moments': True,\n",
       "  'looks': False,\n",
       "  'completely': False,\n",
       "  '2': False,\n",
       "  'reason': False,\n",
       "  'mother': False,\n",
       "  'whose': False,\n",
       "  'line': True,\n",
       "  'night': False,\n",
       "  'human': False,\n",
       "  'rest': True,\n",
       "  'performances': False,\n",
       "  'different': True,\n",
       "  'evil': False,\n",
       "  'small': False,\n",
       "  'james': False,\n",
       "  'simply': True,\n",
       "  'couple': True,\n",
       "  'put': False,\n",
       "  'let': True,\n",
       "  'anyone': False,\n",
       "  'ending': False,\n",
       "  'case': False,\n",
       "  'several': False,\n",
       "  'dead': False,\n",
       "  'michael': False,\n",
       "  'left': False,\n",
       "  'thought': True,\n",
       "  'school': False,\n",
       "  'shows': True,\n",
       "  'humor': True,\n",
       "  'true': False,\n",
       "  'lost': False,\n",
       "  'written': False,\n",
       "  'friend': False,\n",
       "  'entire': False,\n",
       "  'getting': True,\n",
       "  'town': False,\n",
       "  'turns': False,\n",
       "  'soon': False,\n",
       "  'someone': False,\n",
       "  'second': False,\n",
       "  'main': False,\n",
       "  'stars': False,\n",
       "  'found': False,\n",
       "  'use': False,\n",
       "  'problem': False,\n",
       "  'friends': False,\n",
       "  'tv': True,\n",
       "  'top': False,\n",
       "  'name': False,\n",
       "  'begins': False,\n",
       "  'called': False,\n",
       "  'based': True,\n",
       "  'comic': False,\n",
       "  'david': False,\n",
       "  'head': False,\n",
       "  'else': False,\n",
       "  'idea': False,\n",
       "  'either': True,\n",
       "  'wrong': True,\n",
       "  'unfortunately': False,\n",
       "  'later': False,\n",
       "  'final': False,\n",
       "  'hand': False,\n",
       "  'alien': False,\n",
       "  'house': False,\n",
       "  'group': False,\n",
       "  'full': False,\n",
       "  'used': False,\n",
       "  'tries': False,\n",
       "  'often': False,\n",
       "  'war': False,\n",
       "  'sequence': False,\n",
       "  'keep': False,\n",
       "  'turn': False,\n",
       "  'playing': False,\n",
       "  'boy': False,\n",
       "  'behind': False,\n",
       "  'named': False,\n",
       "  'certainly': False,\n",
       "  'live': False,\n",
       "  'believe': False,\n",
       "  'works': False,\n",
       "  'relationship': False,\n",
       "  'face': False,\n",
       "  'hour': False,\n",
       "  'run': False,\n",
       "  'style': True,\n",
       "  'said': True,\n",
       "  'despite': False,\n",
       "  'person': False,\n",
       "  'finally': False,\n",
       "  'shot': False,\n",
       "  'book': False,\n",
       "  'tell': False,\n",
       "  'maybe': False,\n",
       "  'nice': False,\n",
       "  'son': False,\n",
       "  'perfect': False,\n",
       "  'side': True,\n",
       "  'seeing': True,\n",
       "  'able': False,\n",
       "  'finds': False,\n",
       "  'children': False,\n",
       "  'days': False,\n",
       "  'past': False,\n",
       "  'summer': False,\n",
       "  'camera': False,\n",
       "  'including': False,\n",
       "  'mr': True,\n",
       "  'kids': False,\n",
       "  'lives': False,\n",
       "  'directed': False,\n",
       "  'moment': False,\n",
       "  'game': False,\n",
       "  'running': False,\n",
       "  'fight': False,\n",
       "  'supposed': False,\n",
       "  'video': False,\n",
       "  'car': False,\n",
       "  'matter': False,\n",
       "  'kevin': False,\n",
       "  'joe': False,\n",
       "  'lines': False,\n",
       "  'worth': True,\n",
       "  'daughter': False,\n",
       "  'earth': False,\n",
       "  'starts': False,\n",
       "  'need': False,\n",
       "  'entertaining': False,\n",
       "  'white': False,\n",
       "  'start': False,\n",
       "  'writer': False,\n",
       "  'dark': False,\n",
       "  'short': False,\n",
       "  'self': False,\n",
       "  'worst': False,\n",
       "  'nearly': False,\n",
       "  'opening': False,\n",
       "  'try': False,\n",
       "  'upon': False,\n",
       "  'care': False,\n",
       "  'early': False,\n",
       "  'violence': False,\n",
       "  'throughout': False,\n",
       "  'team': False,\n",
       "  'production': False,\n",
       "  'example': False,\n",
       "  'beautiful': False,\n",
       "  'title': False,\n",
       "  'exactly': False,\n",
       "  'jack': False,\n",
       "  'review': False,\n",
       "  'major': False,\n",
       "  'drama': False,\n",
       "  'problems': False,\n",
       "  'sequences': False,\n",
       "  'obvious': False,\n",
       "  'version': False,\n",
       "  'screenplay': False,\n",
       "  'known': True,\n",
       "  'killer': False,\n",
       "  'robert': False,\n",
       "  'disney': False,\n",
       "  'already': False,\n",
       "  'close': True,\n",
       "  'classic': False,\n",
       "  'others': False,\n",
       "  'hit': False,\n",
       "  'kill': False,\n",
       "  'deep': False,\n",
       "  'five': False,\n",
       "  'order': False,\n",
       "  'act': True,\n",
       "  'simple': False,\n",
       "  'fine': False,\n",
       "  'heart': False,\n",
       "  'roles': False,\n",
       "  'jackie': False,\n",
       "  'direction': False,\n",
       "  'eyes': False,\n",
       "  'four': False,\n",
       "  'question': False,\n",
       "  'sort': False,\n",
       "  'sometimes': False,\n",
       "  'knows': False,\n",
       "  'supporting': False,\n",
       "  'coming': True,\n",
       "  'voice': False,\n",
       "  'women': False,\n",
       "  'truly': True,\n",
       "  'save': True,\n",
       "  'jokes': False,\n",
       "  'computer': False,\n",
       "  'child': False,\n",
       "  'boring': False,\n",
       "  'tom': False,\n",
       "  'level': False,\n",
       "  '1': False,\n",
       "  'body': False,\n",
       "  'guys': False,\n",
       "  'genre': False,\n",
       "  'brother': False,\n",
       "  'strong': False,\n",
       "  'stop': False,\n",
       "  'room': False,\n",
       "  'space': False,\n",
       "  'lee': False,\n",
       "  'ends': False,\n",
       "  'beginning': False,\n",
       "  'ship': False,\n",
       "  'york': False,\n",
       "  'attempt': False,\n",
       "  'thriller': False,\n",
       "  'scream': False,\n",
       "  'peter': False,\n",
       "  'husband': False,\n",
       "  'fiction': False,\n",
       "  'happens': False,\n",
       "  'hero': False,\n",
       "  'novel': False,\n",
       "  'note': False,\n",
       "  'hope': False,\n",
       "  'king': False,\n",
       "  'yes': False,\n",
       "  'says': False,\n",
       "  'tells': False,\n",
       "  'quickly': False,\n",
       "  'romantic': False,\n",
       "  'dog': False,\n",
       "  'oscar': False,\n",
       "  'stupid': True,\n",
       "  'possible': False,\n",
       "  'saw': False,\n",
       "  'lead': False,\n",
       "  'career': False,\n",
       "  'murder': False,\n",
       "  'extremely': False,\n",
       "  'manages': False,\n",
       "  'god': False,\n",
       "  'mostly': False,\n",
       "  'wonder': False,\n",
       "  'particularly': False,\n",
       "  'future': False,\n",
       "  'fans': False,\n",
       "  'sound': False,\n",
       "  'worse': False,\n",
       "  'piece': False,\n",
       "  'involving': False,\n",
       "  'de': False,\n",
       "  'appears': False,\n",
       "  'planet': False,\n",
       "  'paul': False,\n",
       "  'involved': False,\n",
       "  'mean': False,\n",
       "  'none': False,\n",
       "  'taking': False,\n",
       "  'hours': False,\n",
       "  'laugh': False,\n",
       "  'police': False,\n",
       "  'sets': False,\n",
       "  'attention': False,\n",
       "  'co': False,\n",
       "  'hell': False,\n",
       "  'eventually': False,\n",
       "  'single': False,\n",
       "  'fall': False,\n",
       "  'falls': False,\n",
       "  'material': False,\n",
       "  'emotional': False,\n",
       "  'power': False,\n",
       "  'late': False,\n",
       "  'lack': False,\n",
       "  'dr': False,\n",
       "  'van': False,\n",
       "  'result': False,\n",
       "  'elements': False,\n",
       "  'meet': False,\n",
       "  'smith': False,\n",
       "  'science': False,\n",
       "  'experience': False,\n",
       "  'bring': False,\n",
       "  'wild': False,\n",
       "  'living': False,\n",
       "  'theater': False,\n",
       "  'interest': False,\n",
       "  'leads': False,\n",
       "  'word': False,\n",
       "  'feature': False,\n",
       "  'battle': False,\n",
       "  'girls': False,\n",
       "  'alone': False,\n",
       "  'obviously': False,\n",
       "  'george': False,\n",
       "  'within': False,\n",
       "  'usually': True,\n",
       "  'enjoy': True,\n",
       "  'guess': False,\n",
       "  'among': False,\n",
       "  'taken': False,\n",
       "  'feeling': False,\n",
       "  'laughs': False,\n",
       "  'aliens': False,\n",
       "  'talk': False,\n",
       "  'chance': False,\n",
       "  'talent': False,\n",
       "  '3': False,\n",
       "  'middle': False,\n",
       "  'number': False,\n",
       "  'easy': False,\n",
       "  'across': False,\n",
       "  'needs': False,\n",
       "  'attempts': False,\n",
       "  'happen': False,\n",
       "  'television': False,\n",
       "  'chris': False,\n",
       "  'deal': False,\n",
       "  'poor': True,\n",
       "  'form': True,\n",
       "  'girlfriend': False,\n",
       "  'viewer': False,\n",
       "  'release': False,\n",
       "  'killed': False,\n",
       "  'forced': False,\n",
       "  'whether': False,\n",
       "  'wonderful': False,\n",
       "  'feels': False,\n",
       "  'oh': False,\n",
       "  'tale': False,\n",
       "  'serious': False,\n",
       "  'expect': False,\n",
       "  'except': False,\n",
       "  'light': False,\n",
       "  'success': False,\n",
       "  'features': False,\n",
       "  'premise': True,\n",
       "  'happy': False,\n",
       "  'words': True,\n",
       "  'leave': False,\n",
       "  'important': False,\n",
       "  'meets': False,\n",
       "  'history': False,\n",
       "  'giving': False,\n",
       "  'crew': False,\n",
       "  'type': False,\n",
       "  'call': False,\n",
       "  'turned': False,\n",
       "  'released': False,\n",
       "  'parents': False,\n",
       "  'art': False,\n",
       "  'impressive': False,\n",
       "  'mission': False,\n",
       "  'working': False,\n",
       "  'seemed': True,\n",
       "  'score': False,\n",
       "  'told': False,\n",
       "  'recent': False,\n",
       "  'robin': False,\n",
       "  'basically': False,\n",
       "  'entertainment': False,\n",
       "  'america': True,\n",
       "  'surprise': False,\n",
       "  'apparently': False,\n",
       "  'easily': False,\n",
       "  'ryan': False,\n",
       "  'cool': False,\n",
       "  'stuff': False,\n",
       "  'cop': False,\n",
       "  'change': False,\n",
       "  'williams': False,\n",
       "  'crime': False,\n",
       "  'office': False,\n",
       "  'parts': False,\n",
       "  'somehow': False,\n",
       "  'sequel': False,\n",
       "  'william': False,\n",
       "  'cut': False,\n",
       "  'die': False,\n",
       "  'jones': False,\n",
       "  'credits': False,\n",
       "  'batman': False,\n",
       "  'suspense': False,\n",
       "  'brings': False,\n",
       "  'events': False,\n",
       "  'reality': False,\n",
       "  'local': False,\n",
       "  'talking': False,\n",
       "  'difficult': False,\n",
       "  'using': False,\n",
       "  'went': True,\n",
       "  'writing': False,\n",
       "  'remember': False,\n",
       "  'near': False,\n",
       "  'straight': False,\n",
       "  'hilarious': False,\n",
       "  'ago': False,\n",
       "  'certain': False,\n",
       "  'ben': False,\n",
       "  'kid': False,\n",
       "  'slow': False,\n",
       "  'blood': False,\n",
       "  'mystery': False,\n",
       "  'complete': False,\n",
       "  'red': False,\n",
       "  'popular': False,\n",
       "  'effective': False,\n",
       "  'fast': False,\n",
       "  'flick': True,\n",
       "  'due': False,\n",
       "  'runs': False,\n",
       "  'gone': True,\n",
       "  'return': False,\n",
       "  'presence': False,\n",
       "  'quality': False,\n",
       "  'dramatic': False,\n",
       "  'filmmakers': False,\n",
       "  'age': False,\n",
       "  'brothers': False,\n",
       "  'business': False,\n",
       "  'general': False,\n",
       "  'rock': False,\n",
       "  'sexual': False,\n",
       "  'present': False,\n",
       "  'surprisingly': False,\n",
       "  'anyway': False,\n",
       "  'uses': False,\n",
       "  '4': False,\n",
       "  'personal': False,\n",
       "  'figure': False,\n",
       "  'smart': False,\n",
       "  'ways': False,\n",
       "  'decides': False,\n",
       "  'annoying': False,\n",
       "  'begin': False,\n",
       "  'somewhat': False,\n",
       "  'shots': False,\n",
       "  'rich': False,\n",
       "  'minute': False,\n",
       "  'law': False,\n",
       "  'previous': False,\n",
       "  'jim': False,\n",
       "  'successful': False,\n",
       "  'harry': False,\n",
       "  'water': False,\n",
       "  'similar': False,\n",
       "  'absolutely': False,\n",
       "  'motion': False,\n",
       "  'former': False,\n",
       "  'strange': False,\n",
       "  'came': False,\n",
       "  'follow': False,\n",
       "  'read': False,\n",
       "  'project': False,\n",
       "  'million': False,\n",
       "  'secret': False,\n",
       "  'starring': False,\n",
       "  'clear': False,\n",
       "  'familiar': False,\n",
       "  'romance': False,\n",
       "  'intelligent': False,\n",
       "  'third': False,\n",
       "  'excellent': False,\n",
       "  'amazing': False,\n",
       "  'party': False,\n",
       "  'budget': False,\n",
       "  'eye': False,\n",
       "  'actress': False,\n",
       "  'prison': False,\n",
       "  'latest': False,\n",
       "  'means': False,\n",
       "  'company': False,\n",
       "  'towards': False,\n",
       "  'predictable': False,\n",
       "  'powerful': False,\n",
       "  'bob': False,\n",
       "  'beyond': False,\n",
       "  'visual': False,\n",
       "  'leaves': False,\n",
       "  'r': False,\n",
       "  'nature': False,\n",
       "  'following': False,\n",
       "  'villain': False,\n",
       "  'leaving': False,\n",
       "  'animated': False,\n",
       "  'low': False,\n",
       "  'b': False,\n",
       "  'bill': False,\n",
       "  'sam': False,\n",
       "  'filled': False,\n",
       "  'wars': False,\n",
       "  'questions': False,\n",
       "  'cinema': False,\n",
       "  'message': False,\n",
       "  'box': False,\n",
       "  'moving': False,\n",
       "  'country': False,\n",
       "  'usual': False,\n",
       "  'martin': False,\n",
       "  'definitely': False,\n",
       "  'add': False,\n",
       "  'large': False,\n",
       "  'clever': False,\n",
       "  'create': False,\n",
       "  'felt': False,\n",
       "  'stories': False,\n",
       "  'brilliant': False,\n",
       "  'ones': False,\n",
       "  'giant': False,\n",
       "  'situation': False,\n",
       "  'murphy': False,\n",
       "  'break': False,\n",
       "  'opens': False,\n",
       "  'scary': False,\n",
       "  'doubt': False,\n",
       "  'drug': False,\n",
       "  'bunch': False,\n",
       "  'thinking': False,\n",
       "  'solid': False,\n",
       "  'effect': True,\n",
       "  'learn': False,\n",
       "  'move': False,\n",
       "  'force': False,\n",
       "  'potential': False,\n",
       "  'seriously': True,\n",
       "  'follows': False,\n",
       "  'saying': False,\n",
       "  'huge': False,\n",
       "  'class': False,\n",
       "  'plan': False,\n",
       "  'agent': False,\n",
       "  'created': False,\n",
       "  'unlike': True,\n",
       "  'pay': False,\n",
       "  'non': False,\n",
       "  'married': False,\n",
       "  'mark': False,\n",
       "  'sweet': False,\n",
       "  'perfectly': False,\n",
       "  'ex': False,\n",
       "  'realize': False,\n",
       "  'audiences': False,\n",
       "  'took': False,\n",
       "  'decent': False,\n",
       "  'likely': False,\n",
       "  'dream': False,\n",
       "  'view': False,\n",
       "  'scott': False,\n",
       "  'subject': False,\n",
       "  'understand': False,\n",
       "  'happened': False,\n",
       "  'enjoyable': False,\n",
       "  'studio': False,\n",
       "  'immediately': False,\n",
       "  'open': False,\n",
       "  'e': False,\n",
       "  'points': False,\n",
       "  'heard': False,\n",
       "  'viewers': False,\n",
       "  'cameron': False,\n",
       "  'truman': False,\n",
       "  'bruce': False,\n",
       "  'frank': False,\n",
       "  'private': False,\n",
       "  'stay': True,\n",
       "  'fails': True,\n",
       "  'impossible': False,\n",
       "  'cold': False,\n",
       "  'richard': False,\n",
       "  'overall': True,\n",
       "  'merely': False,\n",
       "  'exciting': False,\n",
       "  'mess': False,\n",
       "  'chase': False,\n",
       "  'free': False,\n",
       "  'ten': False,\n",
       "  'neither': False,\n",
       "  'wanted': False,\n",
       "  'gun': False,\n",
       "  'appear': False,\n",
       "  'carter': False,\n",
       "  'escape': False,\n",
       "  'ultimately': False,\n",
       "  'fan': False,\n",
       "  'inside': False,\n",
       "  'favorite': False,\n",
       "  'modern': False,\n",
       "  'l': False,\n",
       "  'wedding': False,\n",
       "  'stone': False,\n",
       "  'trek': True,\n",
       "  'brought': False,\n",
       "  'trouble': False,\n",
       "  'otherwise': False,\n",
       "  'tim': False,\n",
       "  '5': False,\n",
       "  'allen': False,\n",
       "  'bond': False,\n",
       "  'society': False,\n",
       "  'liked': True,\n",
       "  'dumb': False,\n",
       "  'musical': False,\n",
       "  'stand': False,\n",
       "  'political': False,\n",
       "  'various': False,\n",
       "  'talented': False,\n",
       "  'particular': False,\n",
       "  'west': False,\n",
       "  'state': True,\n",
       "  'keeps': False,\n",
       "  'english': False,\n",
       "  'silly': False,\n",
       "  'u': False,\n",
       "  'situations': False,\n",
       "  'park': False,\n",
       "  'teen': False,\n",
       "  'rating': False,\n",
       "  'slightly': False,\n",
       "  'steve': False,\n",
       "  'truth': False,\n",
       "  'air': False,\n",
       "  'element': False,\n",
       "  'joke': False,\n",
       "  'spend': False,\n",
       "  'key': False,\n",
       "  'biggest': False,\n",
       "  'members': False,\n",
       "  'effort': False,\n",
       "  'government': False,\n",
       "  'focus': False,\n",
       "  'eddie': False,\n",
       "  'soundtrack': False,\n",
       "  'hands': False,\n",
       "  'earlier': False,\n",
       "  'chan': False,\n",
       "  'purpose': False,\n",
       "  'today': False,\n",
       "  'showing': False,\n",
       "  'memorable': False,\n",
       "  'six': False,\n",
       "  'cannot': False,\n",
       "  'max': False,\n",
       "  'offers': False,\n",
       "  'rated': False,\n",
       "  'mars': False,\n",
       "  'heavy': False,\n",
       "  'totally': False,\n",
       "  'control': False,\n",
       "  'credit': False,\n",
       "  'fi': False,\n",
       "  'woody': False,\n",
       "  'ideas': False,\n",
       "  'sci': False,\n",
       "  'wait': False,\n",
       "  'sit': False,\n",
       "  'female': False,\n",
       "  'ask': False,\n",
       "  'waste': False,\n",
       "  'terrible': False,\n",
       "  'depth': False,\n",
       "  'simon': False,\n",
       "  'aspect': False,\n",
       "  'list': False,\n",
       "  'mary': False,\n",
       "  'sister': False,\n",
       "  'animation': False,\n",
       "  'entirely': False,\n",
       "  'fear': False,\n",
       "  'steven': False,\n",
       "  'moves': False,\n",
       "  'actual': False,\n",
       "  'army': False,\n",
       "  'british': False,\n",
       "  'constantly': False,\n",
       "  'fire': False,\n",
       "  'convincing': False,\n",
       "  'setting': False,\n",
       "  'gave': False,\n",
       "  'tension': False,\n",
       "  'street': False,\n",
       "  '8': False,\n",
       "  'brief': False,\n",
       "  'ridiculous': False,\n",
       "  'cinematography': False,\n",
       "  'typical': False,\n",
       "  'nick': False,\n",
       "  'screenwriter': False,\n",
       "  'ability': False,\n",
       "  'spent': False,\n",
       "  'quick': False,\n",
       "  'violent': False,\n",
       "  'atmosphere': False,\n",
       "  'subtle': False,\n",
       "  'expected': False,\n",
       "  'fairly': True,\n",
       "  'seven': False,\n",
       "  'killing': False,\n",
       "  'tone': False,\n",
       "  'master': False,\n",
       "  'disaster': False,\n",
       "  'lots': False,\n",
       "  'thinks': False,\n",
       "  'song': False,\n",
       "  'cheap': False,\n",
       "  'suddenly': False,\n",
       "  'background': False,\n",
       "  'club': False,\n",
       "  'willis': False,\n",
       "  'whatever': False,\n",
       "  'highly': False,\n",
       "  'sees': False,\n",
       "  'complex': False,\n",
       "  'greatest': False,\n",
       "  'impact': False,\n",
       "  'beauty': False,\n",
       "  'front': False,\n",
       "  'humans': False,\n",
       "  'indeed': False,\n",
       "  'flat': False,\n",
       "  'grace': False,\n",
       "  'wrote': False,\n",
       "  'amusing': False,\n",
       "  'ii': False,\n",
       "  'mike': False,\n",
       "  'cute': False,\n",
       "  'dull': False,\n",
       "  'minor': False,\n",
       "  'recently': False,\n",
       "  'hate': False,\n",
       "  'outside': False,\n",
       "  'plenty': False,\n",
       "  'wish': False,\n",
       "  'godzilla': False,\n",
       "  'college': False,\n",
       "  'titanic': False,\n",
       "  'sounds': False,\n",
       "  'telling': False,\n",
       "  'sight': False,\n",
       "  'double': False,\n",
       "  'cinematic': False,\n",
       "  'queen': False,\n",
       "  'hold': False,\n",
       "  'meanwhile': False,\n",
       "  'awful': False,\n",
       "  'clearly': False,\n",
       "  'theme': False,\n",
       "  'hear': False,\n",
       "  'x': False,\n",
       "  'amount': False,\n",
       "  'baby': False,\n",
       "  'approach': False,\n",
       "  'dreams': False,\n",
       "  'shown': False,\n",
       "  'island': False,\n",
       "  'reasons': False,\n",
       "  'charm': False,\n",
       "  'miss': False,\n",
       "  'longer': False,\n",
       "  'common': False,\n",
       "  'sean': False,\n",
       "  'carry': False,\n",
       "  'believable': False,\n",
       "  'realistic': False,\n",
       "  'chemistry': False,\n",
       "  'possibly': False,\n",
       "  'casting': True,\n",
       "  'carrey': False,\n",
       "  'french': False,\n",
       "  'trailer': False,\n",
       "  'tough': False,\n",
       "  'produced': False,\n",
       "  'imagine': False,\n",
       "  'choice': False,\n",
       "  'ride': True,\n",
       "  'somewhere': False,\n",
       "  'hot': False,\n",
       "  'race': False,\n",
       "  'road': False,\n",
       "  'leader': False,\n",
       "  'thin': False,\n",
       "  'jerry': False,\n",
       "  'slowly': False,\n",
       "  'delivers': False,\n",
       "  'detective': False,\n",
       "  'brown': False,\n",
       "  'jackson': False,\n",
       "  'member': False,\n",
       "  'provide': False,\n",
       "  'president': False,\n",
       "  'puts': False,\n",
       "  'asks': False,\n",
       "  'critics': False,\n",
       "  'appearance': False,\n",
       "  'famous': False,\n",
       "  'okay': False,\n",
       "  'intelligence': False,\n",
       "  'energy': False,\n",
       "  'sent': True,\n",
       "  'spielberg': False,\n",
       "  'development': False,\n",
       "  'etc': False,\n",
       "  'language': False,\n",
       "  'blue': False,\n",
       "  'proves': False,\n",
       "  'vampire': False,\n",
       "  'seemingly': False,\n",
       "  'basic': False,\n",
       "  'caught': False,\n",
       "  ...},\n",
       " 'neg')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Split to training and test set\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "split_on = int(len(data_set)*.8)\n",
    "X_y_train= data_set[:split_on]\n",
    "X_y_test = data_set[split_on:]\n",
    "print (len(X_y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Train model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = nltk.NaiveBayesClassifier.train(X_y_train) # Note: the difference grammar comparing with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Evaluate model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(clf, X_y_test)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review most informative features\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  wasted = True              neg : pos    =     11.4 : 1.0\n",
      "                 details = True              pos : neg    =     11.3 : 1.0\n",
      "                    alas = True              neg : pos    =     10.1 : 1.0\n",
      "                anywhere = True              neg : pos    =      9.3 : 1.0\n",
      "               animation = True              pos : neg    =      8.5 : 1.0\n",
      "                    gary = True              neg : pos    =      8.5 : 1.0\n",
      "                   harry = True              neg : pos    =      8.5 : 1.0\n",
      "             wonderfully = True              pos : neg    =      8.0 : 1.0\n",
      "               pointless = True              neg : pos    =      8.0 : 1.0\n",
      "                 unfunny = True              neg : pos    =      7.5 : 1.0\n",
      "             outstanding = True              pos : neg    =      7.4 : 1.0\n",
      "                 patrick = True              pos : neg    =      7.4 : 1.0\n",
      "                   sheer = True              pos : neg    =      6.9 : 1.0\n",
      "                  strike = True              pos : neg    =      6.9 : 1.0\n",
      "                     las = True              neg : pos    =      6.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "clf.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Using sklearn\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Load data \n",
    "\n",
    "data set ['amazon-reviews-unlocked-mobile-phones'](https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones)\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len=  413,840\n",
      "columns= ['Product Name', 'Brand Name', 'Price', 'Rating', 'Reviews', 'Review Votes']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0  \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0  \n",
       "2       5                                       Very pleased           0.0  \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0  \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd= os.getcwd() # current working directory\n",
    "# path = os.path.join(cwd,'data')\n",
    "fn=  'Amazon_Unlocked_Mobile.csv' # https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones\n",
    "df = pd.read_csv(fn) #\n",
    "print('len=  {:,}\\ncolumns= {}'.format(len(df), list(df)))\n",
    "\n",
    "# df = df.sample(frac=0.1, random_state=10) # reduce the amount of reviews due to speedup the training considering this is demo\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Get rid of records with missed data \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len=  334,328\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "print('len=  {:,}'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Label positive and negative \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Review Votes</th>\n",
       "      <th>Rating_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>I feel so LUCKY to have found this used (phone...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>nice phone, nice up grade from my pantach revu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Very pleased</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>It works good but it goes slow sometimes but i...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>199.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Great phone to replace my lost phone. The only...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Product Name Brand Name   Price  \\\n",
       "0  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "1  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "2  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "3  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "4  \"CLEAR CLEAN ESN\" Sprint EPIC 4G Galaxy SPH-D7...    Samsung  199.99   \n",
       "\n",
       "   Rating                                            Reviews  Review Votes  \\\n",
       "0       5  I feel so LUCKY to have found this used (phone...           1.0   \n",
       "1       4  nice phone, nice up grade from my pantach revu...           0.0   \n",
       "2       5                                       Very pleased           0.0   \n",
       "3       4  It works good but it goes slow sometimes but i...           0.0   \n",
       "4       4  Great phone to replace my lost phone. The only...           0.0   \n",
       "\n",
       "   Rating_binary  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['Rating'] != 3] # Remove any 'neutral' ratings equal to 3  as uninformative\n",
    "df['Rating_binary'] = np.where(df['Rating'] > 3, 1, 0) # returns 1 for 4,5 and 0 for 1,2\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.748269374249846"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Rating_binary'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Split to train and test sets\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Reviews'],df['Rating_binary'],random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review training sample\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Best thing I ever ordered. It's in perfect condition, came with battery and charger. It arrived in just a couple days. I Love It?\",\n",
       " 1)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[0], y_train.iloc[0] # Be careful with quering like X_train[0] because it casts to X_train.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Extract Features \n",
    "\n",
    "</font>\n",
    "The bag-of-words approach is simple way to represent text for use in machine learning, which ignores structure and only counts how often each word occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### CountVectorizer vectorizer\n",
    "\n",
    "</font>\n",
    "By default, selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features samples:\n",
      "['00' '4gig' 'adoption' 'asp' 'blankets' 'casecredit' 'condemned'\n",
      " 'deafult' 'documentation' 'esperiencia' 'fixer' 'goodlots' 'howeveri'\n",
      " 'irc' 'lifethose' 'miamithank' 'niceties' 'ownd' 'political' 'quererme'\n",
      " 'resolucion' 'sel' 'sometes' 'swetingtherefore' 'tote' 'usd79'\n",
      " 'willhappen']\n",
      "\n",
      "len of features 53,415\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer().fit(X_train) # Fit the CountVectorizer to the training data\n",
    "print('features samples:\\n{}'.format(vect.get_feature_names_out()[::2000])) # display each 2000-th feature\n",
    "print ('\\nlen of features {:,}'.format(len(vect.get_feature_names_out())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Transfrom the X_train to feature representation\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<231202x53415 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6117881 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train) # indeces of existing words from vocabulary and their count in current text\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5026)\t1\n",
      "  (0, 5852)\t1\n",
      "  (0, 7198)\t1\n",
      "  (0, 7649)\t1\n",
      "  (0, 9542)\t1\n",
      "  (0, 10426)\t1\n",
      "  (0, 12020)\t1\n",
      "  (0, 12960)\t1\n",
      "  (0, 13915)\t1\n",
      "  (0, 18259)\t1\n",
      "  (0, 24783)\t2\n",
      "  (0, 26171)\t3\n",
      "  (0, 26795)\t1\n",
      "  (0, 28670)\t1\n",
      "  (0, 33488)\t1\n",
      "  (0, 34779)\t1\n",
      "  (0, 47237)\t1\n",
      "  (0, 52135)\t1\n"
     ]
    }
   ],
   "source": [
    "print (X_train_vectorized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review vectorized training sample\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53410</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53411</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53412</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53413</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53414</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53415 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       value\n",
       "0          0\n",
       "1          0\n",
       "2          0\n",
       "3          0\n",
       "4          0\n",
       "...      ...\n",
       "53410      0\n",
       "53411      0\n",
       "53412      0\n",
       "53413      0\n",
       "53414      0\n",
       "\n",
       "[53415 rows x 1 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review first sample\n",
    "df = pd.DataFrame(X_train_vectorized[0].toarray(), index= ['value']).T\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5026, 5852, 7198, 7649, 9542, 10426, 12020, 12960, 13915, 18259, 24783, 26171, 26795, 28670, 33488, 34779, 47237, 52135]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'arrived',\n",
       " 'battery',\n",
       " 'best',\n",
       " 'came',\n",
       " 'charger',\n",
       " 'condition',\n",
       " 'couple',\n",
       " 'days',\n",
       " 'ever',\n",
       " 'in',\n",
       " 'it',\n",
       " 'just',\n",
       " 'love',\n",
       " 'ordered',\n",
       " 'perfect',\n",
       " 'thing',\n",
       " 'with']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (list(df[df['value']>0].index))\n",
    "[vect.get_feature_names_out()[index] for index in df[df['value']>0].index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Train model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=2000).fit(X_train_vectorized, y_train) # Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Evaluate model\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.967568684156166\n",
      "AUC:  0.9793126282763929\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(vect.transform(X_test)) # Predict the transformed test documents\n",
    "print('f1: ', f1_score(y_test, predictions))\n",
    "scores = clf.decision_function(vect.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Review relevant features \n",
    "    \n",
    "</font>\n",
    "\n",
    "The smallest coefs corresponds to `Neg` impact, and largest coefs represent `Pos` impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 53415),\n",
       " (53415,),\n",
       " [-5.153103790597999,\n",
       "  -3.9374596368268757,\n",
       "  -3.748686859339774,\n",
       "  -3.5716910706268763,\n",
       "  -3.318505971547361,\n",
       "  -3.315362076693784,\n",
       "  -3.2701568923671074,\n",
       "  -3.2558265929200565,\n",
       "  -3.1714529653832773,\n",
       "  -3.122313332904471],\n",
       " [3.2289376691643654,\n",
       "  3.2312554160344313,\n",
       "  3.2357179245971217,\n",
       "  3.3113626233690456,\n",
       "  3.326498998919694,\n",
       "  3.386029031214735,\n",
       "  3.499818695136082,\n",
       "  3.6025081700023494,\n",
       "  4.89920909187138,\n",
       "  5.002879160589319])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = np.array(vect.get_feature_names_out())\n",
    "sorted_coef_index = clf.coef_[0].argsort() # ascending  [0] is just squeeze from shape (1,n)\n",
    "clf.coef_.shape, clf.coef_[0].shape, sorted(clf.coef_[0])[:10], sorted(clf.coef_[0])[-11:-1],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest coefs:\n",
      "['mony' 'worst' 'horribly' 'false' 'blacklist' 'lemon' 'messing' 'junk'\n",
      " 'worthless' 'mouth']\n",
      "\n",
      "Largest Coefs: \n",
      "['excelent' '4eeeks' 'excelente' 'excellent' 'loving' 'pleasantly'\n",
      " 'exelente' 'loves' 'lovely' 'buen']\n"
     ]
    }
   ],
   "source": [
    "print('Smallest coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">\n",
    "\n",
    "## Term FrequencyInverse Document Frequency (TF-IDF)\n",
    "\n",
    "</font>\n",
    "\n",
    "TF-IDF is a numerical statistic that reflects how important a word is to a document in a collection or corpus. Its value **increases** with the number of times a word appears in a document and **decreases** with the number of documents in the corpus that contain that word.\n",
    "\n",
    "<div style=\"float:left;\">\n",
    "<br>\n",
    "\n",
    "### Term Frequency\n",
    "\n",
    "The term frequency \\(tf(t,d)\\) measures how often term \\(t\\) appears in document \\(d\\):\n",
    "\n",
    "$$\n",
    "tf(t,d) = \\frac{k}{n}\n",
    "$$  \n",
    "\n",
    "\\(d\\)  document;  \n",
    "\\(k\\)  number of times the word occurs in \\(d\\);  \n",
    "\\(n\\)  total number of words in \\(d\\).\n",
    "\n",
    "*Augmented frequency* (to reduce bias toward longer documents):\n",
    "\n",
    "$$\n",
    "tf^{\\,A}(t,d) = 0.5 + 0.5 \\frac{tf(t,d)}\n",
    "                     {\\max_{t' \\in d} tf(t',d)}\n",
    "$$\n",
    "\n",
    "### Inverse Document Frequency\n",
    "\n",
    "The inverse document frequency \\(idf(t,D)\\) measures how much information the word provides:\n",
    "\n",
    "$$\n",
    "idf(t,D) = \\log \\frac{N}{K}\n",
    "$$  \n",
    "\n",
    "\\(D\\)  the entire collection of documents;  \n",
    "\\(K\\)  number of documents in \\(D\\) that contain the word;  \n",
    "\\(N\\)  total number of documents in \\(D\\).\n",
    "\n",
    "</div>\n",
    "\n",
    "**Note:** Various approaches can be used for inverse document frequency.\n",
    "\n",
    "<div style=\"float:left;\">\n",
    "<table width=\"500\">\n",
    "  <tr>\n",
    "    <th style=\"text-align:center\" bgcolor=\"white\">Document&nbsp;1</th>\n",
    "    <th style=\"text-align:center\" bgcolor=\"white\">Document&nbsp;2</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <table>\n",
    "        <tr><th bgcolor=\"gainsboro\">Term</th><th bgcolor=\"gainsboro\">Count</th></tr>\n",
    "        <tr><td>this</td><td>1</td></tr>\n",
    "        <tr><td>is</td><td>1</td></tr>\n",
    "        <tr><td>a</td><td>2</td></tr>\n",
    "        <tr><td>sample</td><td>1</td></tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td>\n",
    "      <table>\n",
    "        <tr><th bgcolor=\"gainsboro\">Term</th><th bgcolor=\"gainsboro\">Count</th></tr>\n",
    "        <tr><td>this</td><td>1</td></tr>\n",
    "        <tr><td>is</td><td>1</td></tr>\n",
    "        <tr><td>another</td><td>2</td></tr>\n",
    "        <tr><td>example</td><td>3</td></tr>\n",
    "      </table>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "<div style=\"float:left;\">\n",
    "<br>\n",
    "\n",
    "For **this**:\n",
    "\n",
    "$$\n",
    "tf(\\text{\"this\"}, d_{1}) = \\frac{1}{5} = 0.2, \\quad\n",
    "tf(\\text{\"this\"}, d_{2}) = \\frac{1}{7} \\approx 0.14, \\quad\n",
    "idf(\\text{\"this\"}, D) = \\log\\frac{2}{2} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "tfidf(\\text{\"this\"}, d_{1}, D) = 0.2 \\times 0 = 0, \\qquad\n",
    "tfidf(\\text{\"this\"}, d_{2}, D) = 0.14 \\times 0 = 0\n",
    "$$\n",
    "\n",
    "For **example**:\n",
    "\n",
    "$$\n",
    "tf(\\text{\"example\"}, d_{1}) = \\frac{0}{5} = 0, \\quad\n",
    "tf(\\text{\"example\"}, d_{2}) = \\frac{3}{7} \\approx 0.43, \\quad\n",
    "idf(\\text{\"example\"}, D) = \\log\\frac{2}{1} \\approx 0.30\n",
    "$$\n",
    "\n",
    "$$\n",
    "tfidf(\\text{\"example\"}, d_{1}, D) = 0 \\times 0.30 = 0, \\qquad\n",
    "tfidf(\\text{\"example\"}, d_{2}, D) = 0.43 \\times 0.30 \\approx 0.129\n",
    "$$\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Sklearn tfidf\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Compute sklearn tfidf for sample with 2 documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 4, 'is': 2, 'sample': 3, 'another': 0, 'example': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.50154891, 0.70490949, 0.50154891],\n",
       "       [0.53428425, 0.80142637, 0.19007382, 0.        , 0.19007382]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(['this is a sample a', 'this is another example another example example'])\n",
    "tfidf_vectorizer = TfidfVectorizer().fit(X)\n",
    "X_vectorized= tfidf_vectorizer.transform(X)\n",
    "print (tfidf_vectorizer.vocabulary_)\n",
    "X_vectorized.toarray()\n",
    "# conclusion: sklearn uses different variant of computation tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Use sklearn tfidf for Amazon_Unlocked_Mobile documents \n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of features= 17,984\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer= TfidfVectorizer(min_df=5)#.fit(X_train)\n",
    "    # min_df - minimum document count to include the term, default is 1\n",
    "    # you may also set max_features (Int or None) to return just limited number of top tfidf features\n",
    "X_train_vectorized = tfidf_vectorizer.fit_transform(X_train)\n",
    "print ('len of features= {:,}'.format(len(tfidf_vectorizer.get_feature_names_out())))\n",
    "    # Note: min_df=5 caused 17,951  comparing to 53,216 acquired by count vectorizer\n",
    "    # Note: min_df=5 is also available in count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01989095 0.01989095 0.01989095 ... 1.         1.         1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([15230,  1288,  3668, ..., 12144,  9802, 16029])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train_vectorized.shape # (231207, 17951) = (n_documents, n_features)\n",
    "sorted_tfidf_index = X_train_vectorized.max(axis=0).toarray()[0].argsort()\n",
    "    # max(axis=0) means max through all docs - will get the max of each word within all docs\n",
    "    # [0] - just squeezing\n",
    "print (np.sort(X_train_vectorized.max(axis=0).toarray()[0]))\n",
    "sorted_tfidf_index # indices of the most tfidf terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_names  ['00' '000' '0000' ... 'e' 't' '']\n",
      "Smallest tfidf:\n",
      "['storageso' 'aggregration' 'commenter' 'pthalo' 'warmness' '1300'\n",
      " 'seizing' '401p' 'bigtime' 'a10']\n",
      "\n",
      "Largest tfidf: \n",
      "['thnx' 'luis' 'positive' 'exito' 'stats' 'returned' 'heated' 'bueno'\n",
      " 'return' 'exellent']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "print ('feature_names ',feature_names)\n",
    "print('Smallest tfidf:\\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest tfidf: \\n{}'.format(feature_names[sorted_tfidf_index[:-11:-1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "#### Train model on features  extracted by tfidf vectorizer\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.965055851936258\n",
      "AUC:  0.9827034570487878\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=1000).fit(X_train_vectorized, y_train) # Train the model\n",
    "predictions = clf.predict(tfidf_vectorizer.transform(X_test))\n",
    "print('f1: ', f1_score(y_test, predictions))\n",
    "scores = clf.decision_function(tfidf_vectorizer.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: Perfromance is not worse but there are 3 times less amount of features used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['not' 'worst' 'useless' 'waste' 'disappointed' 'terrible' 'return'\n",
      " 'returning' 'poor' 'horrible']\n",
      "\n",
      "Largest Coefs: \n",
      "['love' 'great' 'excellent' 'amazing' 'perfect' 'awesome' 'loves' 'easy'\n",
      " 'perfectly' 'best']\n"
     ]
    }
   ],
   "source": [
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### n-grams\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# the problem is the following reviews are treated the same by current model\n",
    "targets= [\n",
    "    \"not an issue, phone is working\",\n",
    "    \"an issue, phone is not working\"\n",
    "]\n",
    "print(clf.predict(tfidf_vectorizer.transform(targets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of features using n-grams vectorizer=50,000\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=5, max_features=50000, ngram_range=(1,2)).fit(X_train) # Note: both limits are included\n",
    "X_train_vectorized = count_vectorizer.transform(X_train)\n",
    "print('len of features using n-grams vectorizer={:,}'.format(len(count_vectorizer.get_feature_names_out())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1:  0.9827553370154267\n",
      "AUC:  0.9897879145821752\n"
     ]
    }
   ],
   "source": [
    "clf= LogisticRegression(max_iter= 2000).fit(X_train_vectorized, y_train)\n",
    "predictions = clf.predict(count_vectorizer.transform(X_test))\n",
    "print('f1: ', f1_score(y_test, predictions))\n",
    "scores = clf.decision_function(count_vectorizer.transform(X_test))\n",
    "print('AUC: ', roc_auc_score(y_test, scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['no good' 'worst' 'junk' 'garbage' 'not good' 'horrible' 'not happy'\n",
      " 'support from' 'looks ok' 'product good']\n",
      "\n",
      "Largest Coefs: \n",
      "['not bad' 'excelent' 'excelente' 'excellent' 'no problems' 'perfect'\n",
      " 'awesome' 'no issues' 'amazing' 'exelente']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(count_vectorizer.get_feature_names_out())\n",
    "sorted_coef_index = clf.coef_[0].argsort()\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not an issue, phone is working', 'an issue, phone is not working']\n",
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print (targets)\n",
    "print(clf.predict(count_vectorizer.transform(targets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Modern Approaches to Sentiment Analysis (2023-2025)\n",
    "\n",
    "</font>\n",
    "\n",
    "Text analysis has evolved significantly in recent years with the introduction of:\n",
    "\n",
    "1. **Word Embeddings** - Dense vector representations that capture semantic meaning\n",
    "2. **Transformer Models** - Context-aware models that understand sequences and relationships\n",
    "3. **Transfer Learning** - Using pre-trained models on large corpora for specific tasks\n",
    "4. **Few-shot Learning** - Ability to perform with minimal examples\n",
    "\n",
    "Let's explore these techniques using our Amazon reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "### Word Embeddings with Word2Vec\n",
    "\n",
    "</font>\n",
    "\n",
    "Unlike bag-of-words or TF-IDF which treat words as discrete atomic units, word embeddings represent words as continuous vectors where semantically similar words are mapped to nearby points.\n",
    "\n",
    "Key advantages:\n",
    "- Captures semantic relationships between words\n",
    "- Reduces dimensionality compared to one-hot encoding\n",
    "- Words with similar meanings have similar vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in ./.venv/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in ./.venv/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in ./.venv/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./.venv/lib/python3.12/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in ./.venv/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/master/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install and import necessary libraries\n",
    "# !pip install gensim\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22374\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and prepare text data for Word2Vec\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Prepare sentences for Word2Vec (it needs a list of tokenized sentences)\n",
    "X_train_tokenized = [tokenize_text(text) for text in X_train]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(X_train_tokenized,\n",
    "                    vector_size=100,     # Dimensionality of word vectors\n",
    "                    window=5,            # Context window size\n",
    "                    min_count=5,         # Ignore words with frequency below this\n",
    "                    workers=4)           # Number of threads\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv.key_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to 'excellent':\n",
      "outstanding: 0.8038\n",
      "excelent: 0.7889\n",
      "awesome: 0.7719\n",
      "exceptional: 0.7705\n",
      "amazing: 0.7560\n",
      "incredible: 0.7472\n",
      "great: 0.7163\n",
      "superb: 0.7127\n",
      "awsome: 0.6956\n",
      "fantastic: 0.6880\n"
     ]
    }
   ],
   "source": [
    "# Examine the model\n",
    "# Find most similar words to \"excellent\"\n",
    "if \"excellent\" in w2v_model.wv:\n",
    "    similar_words = w2v_model.wv.most_similar(\"excellent\", topn=10)\n",
    "    print(\"Words most similar to 'excellent':\")\n",
    "    for word, score in similar_words:\n",
    "        print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create document vectors by averaging word vectors\n",
    "def document_vector(doc, model):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in model.wv]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[word] for word in doc], axis=0)\n",
    "\n",
    "# Create document vectors for train and test sets\n",
    "X_train_w2v = np.array([document_vector(doc, w2v_model) for doc in X_train_tokenized])\n",
    "X_test_tokenized = [tokenize_text(text) for text in X_test]\n",
    "X_test_w2v = np.array([document_vector(doc, w2v_model) for doc in X_test_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec + Logistic Regression:\n",
      "F1 Score: 0.9437\n",
      "AUC Score: 0.9588\n"
     ]
    }
   ],
   "source": [
    "# Train a logistic regression model on Word2Vec features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "w2v_clf = LogisticRegression(max_iter=1000).fit(X_train_w2v, y_train)\n",
    "w2v_predictions = w2v_clf.predict(X_test_w2v)\n",
    "w2v_scores = w2v_clf.predict_proba(X_test_w2v)[:, 1]\n",
    "\n",
    "print(f\"Word2Vec + Logistic Regression:\")\n",
    "print(f\"F1 Score: {f1_score(y_test, w2v_predictions):.4f}\")\n",
    "print(f\"AUC Score: {roc_auc_score(y_test, w2v_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Transformer-Based Models and Transfer Learning\n",
    "\n",
    "</font>\n",
    "\n",
    "Transformer models like BERT, RoBERTa, and their variants have revolutionized NLP by capturing contextual information and enabling transfer learning from large pre-trained models.\n",
    "\n",
    "Key advantages:\n",
    "- Bidirectional context understanding\n",
    "- Pre-trained on massive text corpora\n",
    "- Can be fine-tuned for specific tasks with relatively small datasets\n",
    "- Captures complex linguistic phenomena\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Install transformers and datasets libraries\n",
    "# !pip install transformers datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Let's use a smaller subset for demonstration purposes\n",
    "sample_size = 5000  # Adjust based on your computational resources\n",
    "indices = np.random.choice(len(X_train), sample_size, replace=False)\n",
    "X_train_sample = X_train.iloc[indices].reset_index(drop=True)\n",
    "y_train_sample = y_train.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "# Prepare datasets in the format expected by Hugging Face\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'text': X_train_sample,\n",
    "    'label': y_train_sample\n",
    "})\n",
    "\n",
    "test_indices = np.random.choice(len(X_test), min(1000, len(X_test)), replace=False)\n",
    "X_test_sample = X_test.iloc[test_indices].reset_index(drop=True)\n",
    "y_test_sample = y_test.iloc[test_indices].reset_index(drop=True)\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'text': X_test_sample,\n",
    "    'label': y_test_sample\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|| 5000/5000 [00:00<00:00, 15191.65 examples/s]\n",
      "Map: 100%|| 1000/1000 [00:00<00:00, 15144.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"  # A smaller, faster version of BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 06:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.689700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.609300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.145300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.253500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.165200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.184400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.144500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.140500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.074200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.059200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.021300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.046400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.064500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.073200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.092000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.007100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.021600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.083500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.033200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=471, training_loss=0.1721426574968102, metrics={'train_runtime': 383.0194, 'train_samples_per_second': 39.163, 'train_steps_per_second': 1.23, 'total_flos': 496752744960000.0, 'train_loss': 0.1721426574968102, 'epoch': 3.0})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    # no_cuda=True  # Force training on CPU\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer (DistilBERT) model results:\n",
      "F1 Score: 0.9656\n",
      "Accuracy: 0.9480\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "predictions = trainer.predict(tokenized_test)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "print(f\"Transformer (DistilBERT) model results:\")\n",
    "print(f\"F1 Score: {f1_score(y_test_sample, preds):.4f}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test_sample, preds):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Zero-Shot and Few-Shot Classification\n",
    "\n",
    "</font>\n",
    "\n",
    "Modern NLP models can perform tasks with minimal or even zero examples, leveraging their pre-training on diverse corpora.\n",
    "\n",
    "- **Zero-shot learning**: Using models to classify text without any labeled examples\n",
    "- **Few-shot learning**: Using only a few examples to adapt to a specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# !pip install transformers>=4.26.0\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Didnt came with the pink case offered. 1 week of use and the screen go bad damaged with strips....\n",
      "Predicted: negative with confidence 0.9798\n",
      "--------------------------------------------------\n",
      "Review: buen vendedor lo recomiendo, estoy 100% satisfecho con mi articulo el articulo es como el descrito p...\n",
      "Predicted: positive with confidence 0.9982\n",
      "--------------------------------------------------\n",
      "Review: Phone had horrible service quality for some odd reason then after 2 weeks of using it the phone just...\n",
      "Predicted: negative with confidence 0.9937\n",
      "--------------------------------------------------\n",
      "Review: I took the phone to Brazil and it would only work with one sim card. The sim card that had worked in...\n",
      "Predicted: negative with confidence 0.9914\n",
      "--------------------------------------------------\n",
      "Review: excellent phone. blackberry is the best mark on the market for smart phones. recommended for those w...\n",
      "Predicted: positive with confidence 0.9959\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot classification with a pre-trained model\n",
    "zero_shot_classifier = pipeline(\"zero-shot-classification\",\n",
    "                                model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Sample a few reviews\n",
    "sample_reviews = X_test.iloc[:5].tolist()\n",
    "candidate_labels = [\"positive\", \"negative\"]\n",
    "\n",
    "# Perform zero-shot classification\n",
    "for review in sample_reviews:\n",
    "    # Truncate long reviews for demonstration\n",
    "    short_review = review[:512]\n",
    "    result = zero_shot_classifier(short_review, candidate_labels)\n",
    "    print(f\"Review: {short_review[:100]}...\")\n",
    "    print(f\"Predicted: {result['labels'][0]} with confidence {result['scores'][0]:.4f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Emotion Analysis: Beyond Positive/Negative\n",
    "\n",
    "</font>\n",
    "\n",
    "Modern sentiment analysis goes beyond binary classification to detect specific emotions and nuances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: Didnt came with the pink case offered. 1 week of use and the screen go bad damaged with strips....\n",
      "Detected emotions: sadness (0.4625), neutral (0.3992)\n",
      "--------------------------------------------------\n",
      "Review: buen vendedor lo recomiendo, estoy 100% satisfecho con mi articulo el articulo es como el descrito p...\n",
      "Detected emotions: joy (0.4326), neutral (0.4130)\n",
      "--------------------------------------------------\n",
      "Review: Phone had horrible service quality for some odd reason then after 2 weeks of using it the phone just...\n",
      "Detected emotions: disgust (0.2515), surprise (0.1910)\n",
      "--------------------------------------------------\n",
      "Review: I took the phone to Brazil and it would only work with one sim card. The sim card that had worked in...\n",
      "Detected emotions: sadness (0.9642), surprise (0.0170)\n",
      "--------------------------------------------------\n",
      "Review: excellent phone. blackberry is the best mark on the market for smart phones. recommended for those w...\n",
      "Detected emotions: joy (0.5244), neutral (0.3515)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Emotion analysis using a pre-trained model\n",
    "emotion_classifier = pipeline(\"text-classification\",\n",
    "                            model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "                            top_k=2)\n",
    "\n",
    "# Sample a few reviews\n",
    "sample_reviews = X_test.iloc[:5].tolist()\n",
    "\n",
    "# Perform emotion analysis\n",
    "for review in sample_reviews:\n",
    "    # Truncate long reviews for demonstration\n",
    "    short_review = review[:512]\n",
    "    result = emotion_classifier(short_review)\n",
    "    print(f\"Review: {short_review[:100]}...\")\n",
    "    print(f\"Detected emotions: {result[0][0]['label']} ({result[0][0]['score']:.4f}), {result[0][1]['label']} ({result[0][1]['score']:.4f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "### Aspect-Based Sentiment Analysis\n",
    "\n",
    "</font>\n",
    "\n",
    "Aspect-based sentiment analysis identifies specific aspects or features mentioned in text and determines the sentiment toward each aspect.\n",
    "\n",
    "For example, in a phone review:\n",
    "- \"The battery life is excellent, but the camera quality is poor.\"\n",
    "\n",
    "We want to extract:\n",
    "- Aspect: battery life, Sentiment: positive\n",
    "- Aspect: camera quality, Sentiment: negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: The screen is bright and clear, but the battery drains too quickly.\n",
      "  Aspect: The screen is bright and clear\n",
      "    Sentiment: Positive (0.9971)\n",
      "  Aspect: but the battery drains too quickly.\n",
      "    Sentiment: Negative (0.8436)\n",
      "--------------------------------------------------\n",
      "Review: This phone has an amazing camera, though it's a bit overpriced.\n",
      "  Aspect: This phone has an amazing camera\n",
      "    Sentiment: Positive (0.9921)\n",
      "  Aspect: though it's a bit overpriced.\n",
      "    Sentiment: Negative (0.8434)\n",
      "--------------------------------------------------\n",
      "Review: The software is intuitive, but it crashes frequently when multitasking.\n",
      "  Aspect: The software is intuitive\n",
      "    Sentiment: Positive (0.9952)\n",
      "  Aspect: but it crashes frequently when multitasking.\n",
      "    Sentiment: Negative (0.9440)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load aspect-based sentiment analysis model\n",
    "absa_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"yangheng/deberta-v3-base-absa-v1.1\",\n",
    "    return_all_scores=True,\n",
    "    use_fast=False\n",
    ")\n",
    "\n",
    "# Example reviews with multiple aspects\n",
    "example_reviews = [\n",
    "    \"The screen is bright and clear, but the battery drains too quickly.\",\n",
    "    \"This phone has an amazing camera, though it's a bit overpriced.\",\n",
    "    \"The software is intuitive, but it crashes frequently when multitasking.\"\n",
    "]\n",
    "\n",
    "# Extract aspects and sentiments (simplified approach)\n",
    "for review in example_reviews:\n",
    "    print(f\"Review: {review}\")\n",
    "\n",
    "    # In a real implementation, we would first extract aspects using NER or other techniques\n",
    "    # This is a simplified version for illustration\n",
    "\n",
    "    # Let's assume we've extracted these aspects\n",
    "    aspects = review.split(\", \")\n",
    "    for aspect in aspects:\n",
    "        result = absa_pipeline(aspect)\n",
    "        print(f\"  Aspect: {aspect}\")\n",
    "        for score in result[0]:\n",
    "            if score['score'] > 0.5:  # Only show confident predictions\n",
    "                print(f\"    Sentiment: {score['label']} ({score['score']:.4f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<font color = green >\n",
    "\n",
    "### Multilingual Sentiment Analysis\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: This product is amazing!\n",
      "Sentiment: 5 stars (0.8755)\n",
      "Spanish: Este producto es increble!\n",
      "Sentiment: 5 stars (0.8487)\n",
      "French: Ce produit est incroyable!\n",
      "Sentiment: 5 stars (0.8649)\n",
      "German: Dieses Produkt ist unglaublich!\n",
      "Sentiment: 5 stars (0.7747)\n",
      "Italian: Questo prodotto  incredibile!\n",
      "Sentiment: 5 stars (0.8987)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load multilingual sentiment model\n",
    "multilingual_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    ")\n",
    "\n",
    "# Sample texts in different languages\n",
    "texts = {\n",
    "    \"English\": \"This product is amazing!\",\n",
    "    \"Spanish\": \"Este producto es increble!\",\n",
    "    \"French\": \"Ce produit est incroyable!\",\n",
    "    \"German\": \"Dieses Produkt ist unglaublich!\",\n",
    "    \"Italian\": \"Questo prodotto  incredibile!\"\n",
    "}\n",
    "\n",
    "# Analyze sentiment across languages\n",
    "for language, text in texts.items():\n",
    "    result = multilingual_classifier(text)\n",
    "    print(f\"{language}: {text}\")\n",
    "    print(f\"Sentiment: {result[0]['label']} ({result[0]['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<font color = green >\n",
    "\n",
    "### Using Large Language Models\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I absolutely love this product! It's perfect for my needs.\n",
      "GPT Analysis: Positive with a confidence score of 0.95\n",
      "--------------------------------------------------\n",
      "Review: This is the worst purchase I've ever made. Complete waste of money.\n",
      "GPT Analysis: Negative with a confidence score of 0.95\n",
      "--------------------------------------------------\n",
      "Review: The product arrived on time and works as expected.\n",
      "GPT Analysis: Positive, 0.95\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# This would require API access to models like OpenAI's GPT\n",
    "import openai\n",
    "\n",
    "# Function to analyze sentiment using GPT\n",
    "def analyze_sentiment_with_gpt(text):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a sentiment analysis assistant. Classify the following text as positive, negative, or neutral, and provide a confidence score between 0 and 1.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Sample usage\n",
    "reviews = [\n",
    "    \"I absolutely love this product! It's perfect for my needs.\",\n",
    "    \"This is the worst purchase I've ever made. Complete waste of money.\",\n",
    "    \"The product arrived on time and works as expected.\"\n",
    "]\n",
    "\n",
    "for review in reviews:\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"GPT Analysis: {analyze_sentiment_with_gpt(review)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green >\n",
    "\n",
    "## Conclusion and Best Practices\n",
    "\n",
    "</font>\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Evolution of Sentiment Analysis**:\n",
    "   - Traditional methods (BoW, TF-IDF) provide baselines and are still useful for simple tasks\n",
    "   - Word embeddings capture semantic relationships\n",
    "   - Transformer models provide state-of-the-art performance\n",
    "\n",
    "2. **Trade-offs**:\n",
    "   - Computational resources vs. accuracy\n",
    "   - Training time vs. performance\n",
    "   - Model size vs. inference speed\n",
    "\n",
    "3. **Choosing the Right Approach**:\n",
    "   - For simple applications with limited resources: TF-IDF + classical ML\n",
    "   - For moderate complexity: Word embeddings + neural networks\n",
    "   - For maximum accuracy: Fine-tuned transformer models\n",
    "   - For specialized applications: Zero-shot or few-shot learning\n",
    "\n",
    "4. **Beyond Binary Sentiment**:\n",
    "   - Emotion detection\n",
    "   - Aspect-based sentiment analysis\n",
    "   - Stance detection\n",
    "   - Sarcasm and irony detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Home Task\n",
    "\n",
    "</font>\n",
    "\n",
    "Objective: Apply at least two (2) modern sentiment analysis methods to classify text data and evaluate their performance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "\n",
    "Sentiment Analysis Dataset: https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\n",
    "\n",
    "alternative source:\n",
    "[rt-polaritydata](https://github.com/dennybritz/cnn-text-classification-tf/tree/master/data/rt-polaritydata)\n",
    "\n",
    "Each line in these two files corresponds to a single snippet (usually containing roughly one single sentence); all snippets are down-cased.\n",
    "\n",
    "[More info about dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.README.1.0.txt)\n",
    "\n",
    "\n",
    "- rt-polarity.neg: Contains negative reviews.\n",
    "- rt-polarity.pos: Contains positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task Description:\n",
    "\n",
    "1. Data Loading & Preparation:\n",
    "\n",
    "    - Load rt-polarity.neg and rt-polarity.pos.\n",
    "    - Split each file into individual snippets.\n",
    "    - Assign labels (0 for negative, 1 for positive).\n",
    "    - Combine into a single dataset.\n",
    "    - Split the dataset into training and testing sets.\n",
    "2. Implement and evaluate at least three (3) methods from the lecture, prioritizing modern approaches.\n",
    "3. For each implemented method, report and compare classification metrics: Accuracy, Precision, Recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "fn='rt-polarity.neg'\n",
    "\n",
    "with open(fn, \"r\",encoding='utf-8', errors='ignore') as f: # some invalid symbols encountered\n",
    "    content = f.read()\n",
    "texts_neg=  content.splitlines()\n",
    "print ('len of texts_neg = {:,}'.format (len(texts_neg)))\n",
    "for review in texts_neg[:5]:\n",
    "    print ( '\\n', review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "fn='rt-polarity.pos'\n",
    "\n",
    "with open(fn, \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    content = f.read()\n",
    "texts_pos=  content.splitlines()\n",
    "print ('len of texts_pos = {:,}'.format (len(texts_pos)))\n",
    "for review in texts_pos[:5]:\n",
    "    print ('\\n', review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font color = green >\n",
    "\n",
    "## Next lesson: Time Series Forecast  \n",
    "</font>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
